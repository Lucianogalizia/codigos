{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6abec2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conectando a Oracle (cx_Oracle)…\n",
      "Leyendo FDP_DINA…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ry16123\\AppData\\Local\\Temp\\ipykernel_41768\\944859134.py:90: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  return pd.read_sql(sql, con=conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leyendo Producción (sum PROD_OIL por NIVEL_4)…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ry16123\\AppData\\Local\\Temp\\ipykernel_41768\\944859134.py:90: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  return pd.read_sql(sql, con=conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leyendo Excel EXPOSICIÓN…\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Worksheet named 'EXPOSICION' not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 267\u001b[0m\n\u001b[0;32m    264\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Exportado \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(final)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m filas → \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout\u001b[38;5;241m.\u001b[39mabsolute()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 267\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 251\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    248\u001b[0m     prod \u001b[38;5;241m=\u001b[39m load_produccion(conn)\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLeyendo Excel EXPOSICIÓN…\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 251\u001b[0m expo \u001b[38;5;241m=\u001b[39m \u001b[43mload_exposicion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEXPO_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEXPO_SHEET\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAplicando joins/limpieza (EXPOSICIÓN)…\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    254\u001b[0m fdp \u001b[38;5;241m=\u001b[39m join_expo(fdp, expo)\n",
      "Cell \u001b[1;32mIn[2], line 78\u001b[0m, in \u001b[0;36mload_exposicion\u001b[1;34m(path, sheet)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_exposicion\u001b[39m(path: \u001b[38;5;28mstr\u001b[39m, sheet: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame:\n\u001b[1;32m---> 78\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msheet\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCantidad\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[0;32m     80\u001b[0m         df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCantidad\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32m~\\CursoML-UDEMY\\env\\lib\\site-packages\\pandas\\io\\excel\\_base.py:508\u001b[0m, in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[0m\n\u001b[0;32m    502\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    503\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine should not be specified when passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    504\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    505\u001b[0m     )\n\u001b[0;32m    507\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 508\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    509\u001b[0m \u001b[43m        \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msheet_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    510\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    511\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    512\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    513\u001b[0m \u001b[43m        \u001b[49m\u001b[43musecols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musecols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    514\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    515\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconverters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconverters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    516\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrue_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrue_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfalse_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfalse_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43mskiprows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskiprows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnrows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43mna_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_default_na\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_default_na\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mna_filter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_filter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdate_parser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_parser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m        \u001b[49m\u001b[43mthousands\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthousands\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecimal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecimal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcomment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mskipfooter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipfooter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    531\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    532\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    533\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    534\u001b[0m     \u001b[38;5;66;03m# make sure to close opened file handles\u001b[39;00m\n\u001b[0;32m    535\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m should_close:\n",
      "File \u001b[1;32m~\\CursoML-UDEMY\\env\\lib\\site-packages\\pandas\\io\\excel\\_base.py:1616\u001b[0m, in \u001b[0;36mExcelFile.parse\u001b[1;34m(self, sheet_name, header, names, index_col, usecols, converters, true_values, false_values, skiprows, nrows, na_values, parse_dates, date_parser, date_format, thousands, comment, skipfooter, dtype_backend, **kwds)\u001b[0m\n\u001b[0;32m   1576\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(\n\u001b[0;32m   1577\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1578\u001b[0m     sheet_name: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1596\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds,\n\u001b[0;32m   1597\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, DataFrame] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mint\u001b[39m, DataFrame]:\n\u001b[0;32m   1598\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1599\u001b[0m \u001b[38;5;124;03m    Parse specified sheet(s) into a DataFrame.\u001b[39;00m\n\u001b[0;32m   1600\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1614\u001b[0m \u001b[38;5;124;03m    >>> file.parse()  # doctest: +SKIP\u001b[39;00m\n\u001b[0;32m   1615\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1616\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39mparse(\n\u001b[0;32m   1617\u001b[0m         sheet_name\u001b[38;5;241m=\u001b[39msheet_name,\n\u001b[0;32m   1618\u001b[0m         header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[0;32m   1619\u001b[0m         names\u001b[38;5;241m=\u001b[39mnames,\n\u001b[0;32m   1620\u001b[0m         index_col\u001b[38;5;241m=\u001b[39mindex_col,\n\u001b[0;32m   1621\u001b[0m         usecols\u001b[38;5;241m=\u001b[39musecols,\n\u001b[0;32m   1622\u001b[0m         converters\u001b[38;5;241m=\u001b[39mconverters,\n\u001b[0;32m   1623\u001b[0m         true_values\u001b[38;5;241m=\u001b[39mtrue_values,\n\u001b[0;32m   1624\u001b[0m         false_values\u001b[38;5;241m=\u001b[39mfalse_values,\n\u001b[0;32m   1625\u001b[0m         skiprows\u001b[38;5;241m=\u001b[39mskiprows,\n\u001b[0;32m   1626\u001b[0m         nrows\u001b[38;5;241m=\u001b[39mnrows,\n\u001b[0;32m   1627\u001b[0m         na_values\u001b[38;5;241m=\u001b[39mna_values,\n\u001b[0;32m   1628\u001b[0m         parse_dates\u001b[38;5;241m=\u001b[39mparse_dates,\n\u001b[0;32m   1629\u001b[0m         date_parser\u001b[38;5;241m=\u001b[39mdate_parser,\n\u001b[0;32m   1630\u001b[0m         date_format\u001b[38;5;241m=\u001b[39mdate_format,\n\u001b[0;32m   1631\u001b[0m         thousands\u001b[38;5;241m=\u001b[39mthousands,\n\u001b[0;32m   1632\u001b[0m         comment\u001b[38;5;241m=\u001b[39mcomment,\n\u001b[0;32m   1633\u001b[0m         skipfooter\u001b[38;5;241m=\u001b[39mskipfooter,\n\u001b[0;32m   1634\u001b[0m         dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1635\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds,\n\u001b[0;32m   1636\u001b[0m     )\n",
      "File \u001b[1;32m~\\CursoML-UDEMY\\env\\lib\\site-packages\\pandas\\io\\excel\\_base.py:773\u001b[0m, in \u001b[0;36mBaseExcelReader.parse\u001b[1;34m(self, sheet_name, header, names, index_col, usecols, dtype, true_values, false_values, skiprows, nrows, na_values, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, dtype_backend, **kwds)\u001b[0m\n\u001b[0;32m    770\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReading sheet \u001b[39m\u001b[38;5;132;01m{\u001b[39;00masheetname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(asheetname, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 773\u001b[0m     sheet \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_sheet_by_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43masheetname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# assume an integer if not a string\u001b[39;00m\n\u001b[0;32m    775\u001b[0m     sheet \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_sheet_by_index(asheetname)\n",
      "File \u001b[1;32m~\\CursoML-UDEMY\\env\\lib\\site-packages\\pandas\\io\\excel\\_openpyxl.py:582\u001b[0m, in \u001b[0;36mOpenpyxlReader.get_sheet_by_name\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    581\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_sheet_by_name\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 582\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_if_bad_sheet_by_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    583\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbook[name]\n",
      "File \u001b[1;32m~\\CursoML-UDEMY\\env\\lib\\site-packages\\pandas\\io\\excel\\_base.py:624\u001b[0m, in \u001b[0;36mBaseExcelReader.raise_if_bad_sheet_by_name\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_if_bad_sheet_by_name\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msheet_names:\n\u001b[1;32m--> 624\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWorksheet named \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Worksheet named 'EXPOSICION' not found"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Matriz de Criticidad de AIBs (fuera de Power BI) - versión cx_Oracle\n",
    "# Lee Oracle + Excel EXPOSICIÓN, aplica las mismas transformaciones de Power Query (M)\n",
    "# y clona las medidas DAX compartidas. Exporta Matriz_AIB_export.xlsx.\n",
    "\n",
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cx_Oracle\n",
    "\n",
    "# ================== CREDENCIALES / RUTAS (texto plano) ==================\n",
    "ORA_USER    = \"RY16123\"\n",
    "ORA_PASS    = \"Luciano284\"\n",
    "ORA_HOST    = \"slplpgmoora03\"\n",
    "ORA_PORT    = 1527\n",
    "ORA_SERVICE = \"psfu\"\n",
    "\n",
    "EXPO_PATH   = r\"C:\\Users\\ry16123\\Downloads\\EXPOSICION-new 1.xlsx\"\n",
    "EXPO_SHEET  = \"EXPOSICION\"   # nombre de hoja/tabla dentro del Excel\n",
    "\n",
    "# ================== QUERIES (según tu M) ==================\n",
    "SQL_FDP_DINA = \"\"\"\n",
    "SELECT DBU_FIC_ORG_ESTRUCTURAL.NOMBRE_CORTO,\n",
    "       DBU_FIC_ORG_ESTRUCTURAL.NIVEL_1,\n",
    "       DBU_FIC_ORG_ESTRUCTURAL.NIVEL_2,\n",
    "       DBU_FIC_ORG_ESTRUCTURAL.NIVEL_3,\n",
    "       DBU_FIC_ORG_ESTRUCTURAL.NIVEL_4,\n",
    "       DBU_FIC_ORG_ESTRUCTURAL.NIVEL_5,\n",
    "       FIC_ULTIMO_DINAMOMETRO.FECHA_HORA,\n",
    "       FIC_ULTIMO_DINAMOMETRO.REALIZADO_POR,\n",
    "       FIC_ULTIMO_DINAMOMETRO.AIB_MARCA_Y_DESC_API,\n",
    "       FIC_ULTIMO_DINAMOMETRO.AIBRR_TORQUE_MAXIMO_REDUCTOR,\n",
    "       FIC_ULTIMO_DINAMOMETRO.AIBEB_TORQUE_MAXIMO_REDUCTOR,\n",
    "       FIC_ULTIMO_DINAMOMETRO.AIBRR_TORQUE_DISPONIBLE,\n",
    "       FIC_ULTIMO_DINAMOMETRO.BBA_LLENADO_DE_BOMBA/100 AS LLENAD_BOMBA_PCT,\n",
    "       FIC_ULTIMO_DINAMOMETRO.AIBRE_SOLICITACION_DE_ESTRUCT/100 AS ESTRUCTURA_PCT,\n",
    "       VTOW_WELL_LAST_CONTROL_DET.PROD_OIL,\n",
    "       VTOW_WELL_LAST_CONTROL_DET.PROD_GAS,\n",
    "       VTOW_WELL_LAST_CONTROL_DET.PROD_WAT,\n",
    "       FIC_ULTIMO_DINAMOMETRO.AIB_GPM,\n",
    "       FIC_ULTIMO_DINAMOMETRO.AIB_DIAMETRO_POLEA_REDUCTOR,\n",
    "       FIC_ULTIMO_DINAMOMETRO.MOTOR_DIAMETRO_POLEA\n",
    "FROM DISC_ADMINS.DBU_FIC_ORG_ESTRUCTURAL DBU_FIC_ORG_ESTRUCTURAL,\n",
    "     DISC_ADMINS.VTOW_WELL_LAST_CONTROL_DET VTOW_WELL_LAST_CONTROL_DET,\n",
    "     DISC_ADMINS.FIC_ULTIMO_DINAMOMETRO FIC_ULTIMO_DINAMOMETRO\n",
    "WHERE ((VTOW_WELL_LAST_CONTROL_DET.COMP_SK = DBU_FIC_ORG_ESTRUCTURAL.COMP_SK)\n",
    "       AND (FIC_ULTIMO_DINAMOMETRO.COMP_SK = DBU_FIC_ORG_ESTRUCTURAL.COMP_SK))\n",
    "  AND (DBU_FIC_ORG_ESTRUCTURAL.ESTADO = 'Produciendo')\n",
    "  AND ((FIC_ULTIMO_DINAMOMETRO.AIBRE_SOLICITACION_DE_ESTRUCT/100) <> 0)\n",
    "  AND ((FIC_ULTIMO_DINAMOMETRO.BBA_LLENADO_DE_BOMBA/100) <> 0)\n",
    "  AND (DBU_FIC_ORG_ESTRUCTURAL.MET_PROD = 'Bombeo Mecánico')\n",
    "\"\"\"\n",
    "\n",
    "# Consulta \"Producción\" de tu M: UPPER(NIVEL_4), SUM(PROD_OIL) agrupado por NIVEL_4\n",
    "SQL_PRODUCCION = \"\"\"\n",
    "SELECT UPPER(DBU_FIC_ORG_ESTRUCTURAL.NIVEL_4) AS NIVEL_4,\n",
    "       SUM(VTOW_WELL_LAST_CONTROL_DET.PROD_OIL) AS PROD_OIL_1\n",
    "FROM DISC_ADMINS.DBU_FIC_ORG_ESTRUCTURAL DBU_FIC_ORG_ESTRUCTURAL,\n",
    "     DISC_ADMINS.VTOW_WELL_LAST_CONTROL_DET VTOW_WELL_LAST_CONTROL_DET,\n",
    "     DISC_ADMINS.FIC_ULTIMO_DINAMOMETRO FIC_ULTIMO_DINAMOMETRO\n",
    "WHERE ((VTOW_WELL_LAST_CONTROL_DET.COMP_SK = DBU_FIC_ORG_ESTRUCTURAL.COMP_SK)\n",
    "       AND (FIC_ULTIMO_DINAMOMETRO.COMP_SK = DBU_FIC_ORG_ESTRUCTURAL.COMP_SK))\n",
    "  AND (DBU_FIC_ORG_ESTRUCTURAL.ESTADO = 'Produciendo')\n",
    "  AND ((FIC_ULTIMO_DINAMOMETRO.AIBRE_SOLICITACION_DE_ESTRUCT/100) <> 0)\n",
    "  AND ((FIC_ULTIMO_DINAMOMETRO.BBA_LLENADO_DE_BOMBA/100) <> 0)\n",
    "  AND (DBU_FIC_ORG_ESTRUCTURAL.MET_PROD = 'Bombeo Mecánico')\n",
    "GROUP BY UPPER(DBU_FIC_ORG_ESTRUCTURAL.NIVEL_4)\n",
    "\"\"\"\n",
    "\n",
    "# ================== CONEXIÓN ORACLE (cx_Oracle) ==================\n",
    "def get_connection():\n",
    "    dsn = cx_Oracle.makedsn(ORA_HOST, ORA_PORT, service_name=ORA_SERVICE)\n",
    "    return cx_Oracle.connect(ORA_USER, ORA_PASS, dsn, encoding=\"UTF-8\")\n",
    "\n",
    "# ================== EXPO (traducción de tu M) ==================\n",
    "def load_exposicion(path: str, sheet: str) -> pd.DataFrame:\n",
    "    df = pd.read_excel(path, sheet_name=sheet)\n",
    "    if \"Cantidad\" in df.columns:\n",
    "        df = df.drop(columns=[\"Cantidad\"])\n",
    "    df[\"Denominacion API\"] = df[\"Denominacion API\"].astype(str).str.upper().str.strip()\n",
    "    # Asegurar columnas esperadas\n",
    "    for c in [\"Marca\",\"Tipo\",\"Torque max\",\"SENSIBILIDAD CR\",\"SENSIBILIDAD EST\"]:\n",
    "        if c not in df.columns:\n",
    "            df[c] = np.nan\n",
    "    return df\n",
    "\n",
    "# ================== FDP_DINA y Producción (via cx_Oracle) ==================\n",
    "def read_sql_df(conn, sql: str) -> pd.DataFrame:\n",
    "    return pd.read_sql(sql, con=conn)\n",
    "\n",
    "def load_fdp(conn) -> pd.DataFrame:\n",
    "    df = read_sql_df(conn, SQL_FDP_DINA)\n",
    "    # Text.Upper(NIVEL_4)\n",
    "    df[\"NIVEL_4\"] = df[\"NIVEL_4\"].astype(str).str.upper()\n",
    "    return df\n",
    "\n",
    "def load_produccion(conn) -> pd.DataFrame:\n",
    "    return read_sql_df(conn, SQL_PRODUCCION)  # columnas: NIVEL_4, PROD_OIL_1\n",
    "\n",
    "# ================== Transformaciones equivalentes a Power Query ==================\n",
    "def join_expo(df_fdp: pd.DataFrame, expo: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df_fdp.copy()\n",
    "    df[\"__API_KEY__\"] = df[\"AIB_MARCA_Y_DESC_API\"].astype(str).str.upper().str.strip()\n",
    "    ex = expo.copy()\n",
    "    ex[\"__API_KEY__\"] = ex[\"Denominacion API\"]\n",
    "\n",
    "    df = df.merge(\n",
    "        ex[[\"__API_KEY__\",\"Denominacion API\",\"Marca\",\"Tipo\",\"Torque max\",\"SENSIBILIDAD CR\",\"SENSIBILIDAD EST\"]],\n",
    "        how=\"left\", on=\"__API_KEY__\"\n",
    "    ).drop(columns=[\"__API_KEY__\"])\n",
    "\n",
    "    # Renombres como en Power Query (prefijo EXPOSICIÓN.)\n",
    "    df = df.rename(columns={\n",
    "        \"Denominacion API\": \"EXPOSICIÓN.Denominacion API\",\n",
    "        \"Marca\": \"EXPOSICIÓN.Marca\",\n",
    "        \"Tipo\": \"EXPOSICIÓN.Tipo\",\n",
    "        \"Torque max\": \"EXPOSICIÓN.Torque max\",\n",
    "        \"SENSIBILIDAD CR\": \"EXPOSICIÓN.SENSIBILIDAD CR\",\n",
    "        \"SENSIBILIDAD EST\": \"EXPOSICIÓN.SENSIBILIDAD EST\",\n",
    "    })\n",
    "\n",
    "    # ID = NOMBRE_CORTO & NIVEL_5 → filtrar nulos/vacíos, quitar duplicados por ID\n",
    "    df[\"ID_tmp\"] = (df[\"NOMBRE_CORTO\"].fillna(\"\").astype(str) + df[\"NIVEL_5\"].fillna(\"\").astype(str))\n",
    "    df = df[(df[\"ID_tmp\"]!=\"\") & df[\"NOMBRE_CORTO\"].notna()]\n",
    "    df = df.drop_duplicates(subset=[\"ID_tmp\"]).drop(columns=[\"ID_tmp\"])\n",
    "\n",
    "    # Reemplazos de nulos por 0 en torques\n",
    "    for c in [\"AIBRR_TORQUE_MAXIMO_REDUCTOR\",\"AIBEB_TORQUE_MAXIMO_REDUCTOR\"]:\n",
    "        df[c] = df[c].fillna(0)\n",
    "\n",
    "    # Reemplazo Tierra del Fuego → Chubut\n",
    "    df[\"NIVEL_2\"] = df[\"NIVEL_2\"].replace({\"Tierra del Fuego\": \"Chubut\"})\n",
    "\n",
    "    return df\n",
    "\n",
    "def attach_produccion(df: pd.DataFrame, prod: pd.DataFrame) -> pd.DataFrame:\n",
    "    return df.merge(prod, how=\"left\", on=\"NIVEL_4\")\n",
    "\n",
    "# ================== Medidas/derivadas (clon DAX) ==================\n",
    "def add_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    d = df.copy()\n",
    "\n",
    "    # Ratio_Produccion\n",
    "    d[\"PROD_OIL_1\"] = d[\"PROD_OIL_1\"].replace({0: np.nan})\n",
    "    d[\"Ratio_Produccion\"] = d[\"PROD_OIL\"] / d[\"PROD_OIL_1\"]\n",
    "\n",
    "    # CONSECUENCIA (texto por cortes)\n",
    "    def seg_consecuencia(x):\n",
    "        if pd.isna(x): return \"0 - 0,0025\"\n",
    "        if x >= 0.0065: return \"Mayor a 0,0065\"\n",
    "        if x >= 0.0025: return \"0,0025 - 0,0065\"\n",
    "        return \"0 - 0,0025\"\n",
    "    d[\"f_CONSECUENCIA\"] = d[\"Ratio_Produccion\"].apply(seg_consecuencia)\n",
    "    d[\"CONSECUENCIA\"]   = d[\"f_CONSECUENCIA\"]\n",
    "\n",
    "    # TORQUE_BAL[%] y TORQUE[%]\n",
    "    d[\"TORQUE_BAL[%]\"] = d[\"AIBEB_TORQUE_MAXIMO_REDUCTOR\"] / d[\"AIBRR_TORQUE_DISPONIBLE\"]\n",
    "    d[\"TORQUE[%]\"]     = d[\"AIBRR_TORQUE_MAXIMO_REDUCTOR\"] / d[\"AIBRR_TORQUE_DISPONIBLE\"]\n",
    "\n",
    "    # Factores y EXIGENCIA (según tu M)\n",
    "    d[\"f_CR\"]  = np.select([d[\"TORQUE[%]\"] > 1, d[\"TORQUE[%]\"] >= 0.85], [10, 3.7], default=0)\n",
    "    d[\"f_CR3\"] = d[\"EXPOSICIÓN.SENSIBILIDAD CR\"].map({\"ALTA\":3, \"MEDIA\":1.5}).fillna(1.0)\n",
    "    d[\"CAJA REDUCTORA\"]  = d[\"f_CR\"] * d[\"f_CR3\"]\n",
    "    d[\"CAJA REDUCTORA_\"] = np.where(d[\"TORQUE_BAL[%]\"]>1, 1.5*d[\"CAJA REDUCTORA\"], d[\"CAJA REDUCTORA\"])\n",
    "\n",
    "    d[\"f_estr\"]  = np.select([d[\"ESTRUCTURA_PCT\"] >= 0.95, d[\"ESTRUCTURA_PCT\"] > 0.85], [20,5], default=1)\n",
    "    d[\"f_estr3\"] = d[\"EXPOSICIÓN.SENSIBILIDAD EST\"].map({\"ALTA\":3,\"MEDIA\":1.5}).fillna(1.0)\n",
    "    d[\"ESTRUCTURA\"] = d[\"f_estr\"] * d[\"f_estr3\"]\n",
    "\n",
    "    d[\"f_GPM\"]  = np.select([d[\"AIB_GPM\"] > 9, d[\"AIB_GPM\"] > 7], [3, 1.5], default=1)\n",
    "    d[\"f_GPM2\"] = np.where(d[\"LLENAD_BOMBA_PCT\"] < 0.75, d[\"f_GPM\"]*2, d[\"f_GPM\"])\n",
    "\n",
    "    d[\"EXIGENCIA\"]   = d[\"f_GPM2\"] * d[\"ESTRUCTURA\"] + d[\"CAJA REDUCTORA_\"]\n",
    "    d[\"F_EXIGENCIA\"] = np.select([d[\"EXIGENCIA\"]>10, d[\"EXIGENCIA\"]>4], [\"A\",\"M\"], default=\"B\")\n",
    "\n",
    "    # CRITICIDAD (reglas de tu M)\n",
    "    def crit(row):\n",
    "        rp = row[\"Ratio_Produccion\"]; fe = row[\"F_EXIGENCIA\"]\n",
    "        if pd.isna(rp) or pd.isna(fe): return \"NORMAL\"\n",
    "        if rp >= 0.0065 and fe == \"A\": return \"CRITICO\"\n",
    "        if rp >= 0.0025 and fe == \"A\": return \"ALERTA\"\n",
    "        if rp <  0.0025 and fe == \"A\": return \"ALERTA\"\n",
    "        if rp >= 0.0065 and fe == \"M\": return \"ALERTA\"\n",
    "        if rp >= 0.0025 and fe == \"M\": return \"ALERTA\"\n",
    "        if rp <  0.0025 and fe == \"M\": return \"ALERTA\"\n",
    "        return \"NORMAL\"\n",
    "    d[\"CRITICIDAD\"] = d.apply(crit, axis=1).astype(str)\n",
    "    d[\"ref_criticidad\"] = d[\"CRITICIDAD\"].map({\"CRITICO\":1, \"ALERTA\":2, \"NORMAL\":3}).astype(\"Int64\")\n",
    "\n",
    "    # Color Detalle CRITICIDAD (DAX)\n",
    "    def color_detalle(rp, fe):\n",
    "        if pd.isna(rp) or pd.isna(fe): return \"\"\n",
    "        if rp >= 0.0065 and fe == \"A\": return \"RED\"\n",
    "        if rp >= 0.0025 and fe == \"A\": return \"ORANGE\"\n",
    "        if rp <  0.0025 and fe == \"A\": return \"ORANGE\"\n",
    "        if rp >= 0.0065 and fe == \"M\": return \"YELLOW\"\n",
    "        if rp >= 0.0025 and fe == \"M\": return \"YELLOW\"\n",
    "        if rp <  0.0025 and fe == \"M\": return \"YELLOW\"\n",
    "        if rp >= 0.0065 and fe == \"B\": return \"GREEN\"\n",
    "        if rp >= 0.0025 and fe == \"B\": return \"GREEN\"\n",
    "        if rp <  0.0025 and fe == \"B\": return \"GREEN\"\n",
    "        return \"\"\n",
    "    d[\"Color Detalle CRITICIDAD\"] = np.vectorize(color_detalle)(d[\"Ratio_Produccion\"], d[\"F_EXIGENCIA\"])\n",
    "\n",
    "    # criticidad color / texto (DAX)\n",
    "    d[\"criticidad color\"] = d[\"CRITICIDAD\"].map({\"CRITICO\":\"Red\", \"ALERTA\":\"YELLOW\", \"NORMAL\":\"Green\"}).fillna(\"White\")\n",
    "    d[\"criticidad texto\"] = d[\"CRITICIDAD\"].map({\"CRITICO\":\"white\", \"ALERTA\":\"black\", \"NORMAL\":\"white\"}).fillna(\"White\")\n",
    "\n",
    "    # DETALLE CRITICIDAD (DAX)\n",
    "    def detalle_crit(rp, fe):\n",
    "        if pd.isna(rp) or pd.isna(fe): return \"NORMAL\"\n",
    "        if rp >= 0.0065 and fe == \"A\": return \"AA\"\n",
    "        if rp >= 0.0025 and fe == \"A\": return \"AM\"\n",
    "        if rp <  0.0025 and fe == \"A\": return \"AB\"\n",
    "        if rp >= 0.0065 and fe == \"M\": return \"MA\"\n",
    "        if rp >= 0.0025 and fe == \"M\": return \"MM\"\n",
    "        if rp <  0.0025 and fe == \"M\": return \"MB\"\n",
    "        if rp >= 0.0065 and fe == \"B\": return \"BA\"\n",
    "        if rp >= 0.0025 and fe == \"B\": return \"BM\"\n",
    "        if rp <  0.0025 and fe == \"B\": return \"BB\"\n",
    "        return \"NORMAL\"\n",
    "    d[\"DETALLE CRITICIDAD\"] = np.vectorize(detalle_crit)(d[\"Ratio_Produccion\"], d[\"F_EXIGENCIA\"])\n",
    "\n",
    "    # Días sin dina = DATEDIFF(MAX(FECHA_HORA), TODAY(), DAY) → MAX por pozo\n",
    "    d[\"FECHA_HORA\"] = pd.to_datetime(d[\"FECHA_HORA\"], errors=\"coerce\")\n",
    "    max_por_pozo = d.groupby(\"NOMBRE_CORTO\", dropna=False)[\"FECHA_HORA\"].max().rename(\"FECHA_MAX\").reset_index()\n",
    "    d = d.merge(max_por_pozo, on=\"NOMBRE_CORTO\", how=\"left\")\n",
    "    d[\"Días sin dina\"] = (pd.Timestamp.today().normalize() - d[\"FECHA_MAX\"].dt.normalize()).dt.days\n",
    "\n",
    "    # Columnas calculadas dentro de FDP_DINA\n",
    "    d[\"Correlativo Exigencia\"] = d[\"F_EXIGENCIA\"].map({\"A\":\"10 +\", \"M\":\"04-10\", \"B\":\"0-04\"}).fillna(\"\")\n",
    "    d[\"ID\"] = d[\"f_CONSECUENCIA\"].fillna(\"\").astype(str) + d[\"Correlativo Exigencia\"].fillna(\"\").astype(str)\n",
    "\n",
    "    # Filtro final igual que M\n",
    "    d = d[(d[\"NOMBRE_CORTO\"].notna()) & (d[\"NOMBRE_CORTO\"].astype(str)!=\"\")]\n",
    "\n",
    "    return d\n",
    "\n",
    "# ================== PIPELINE ==================\n",
    "def main():\n",
    "    print(\"Conectando a Oracle (cx_Oracle)…\")\n",
    "    with get_connection() as conn:\n",
    "        print(\"Leyendo FDP_DINA…\")\n",
    "        fdp = load_fdp(conn)\n",
    "\n",
    "        print(\"Leyendo Producción (sum PROD_OIL por NIVEL_4)…\")\n",
    "        prod = load_produccion(conn)\n",
    "\n",
    "    print(\"Leyendo Excel EXPOSICIÓN…\")\n",
    "    expo = load_exposicion(EXPO_PATH, EXPO_SHEET)\n",
    "\n",
    "    print(\"Aplicando joins/limpieza (EXPOSICIÓN)…\")\n",
    "    fdp = join_expo(fdp, expo)\n",
    "\n",
    "    print(\"Adjuntando Producción…\")\n",
    "    fdp = attach_produccion(fdp, prod)\n",
    "\n",
    "    print(\"Calculando métricas / criticidad…\")\n",
    "    final = add_features(fdp)\n",
    "\n",
    "    out = Path(\"Matriz_AIB_export.xlsx\")\n",
    "    final.to_excel(out, index=False)\n",
    "    print(f\"✅ Exportado {len(final):,} filas → {out.absolute()}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0397719b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conectando a Oracle (cx_Oracle)…\n",
      "Leyendo FDP_DINA…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ry16123\\AppData\\Local\\Temp\\ipykernel_41768\\1230803187.py:78: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  return pd.read_sql(sql, con=conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leyendo Producción (sum PROD_OIL por NIVEL_4)…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ry16123\\AppData\\Local\\Temp\\ipykernel_41768\\1230803187.py:78: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  return pd.read_sql(sql, con=conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leyendo Excel EXPOSICIÓN…\n",
      "Hojas encontradas en 'C:\\Users\\ry16123\\Downloads\\EXPOSICION-new 1.xlsx': ['Hoja5', 'EXPOSICIÓN', 'Hoja1', 'Hoja2', 'IDP', 'Hoja3', 'OBSERVACIONES']\n",
      "Aplicando joins/limpieza (EXPOSICIÓN)…\n",
      "Adjuntando Producción…\n",
      "Calculando métricas / criticidad…\n",
      "✅ Exportado 3,767 filas → C:\\Users\\ry16123\\Matriz_AIB_export.xlsx\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Matriz de Criticidad de AIBs (fuera de Power BI) - versión cx_Oracle\n",
    "# Lee Oracle + Excel EXPOSICIÓN, aplica transformaciones equivalentes a Power Query (M)\n",
    "# y clona las medidas DAX que compartiste. Exporta Matriz_AIB_export.xlsx.\n",
    "\n",
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cx_Oracle\n",
    "\n",
    "# ================== CREDENCIALES / RUTAS (texto plano, como pediste) ==================\n",
    "ORA_USER    = \"RY16123\"\n",
    "ORA_PASS    = \"Luciano284\"\n",
    "ORA_HOST    = \"slplpgmoora03\"\n",
    "ORA_PORT    = 1527\n",
    "ORA_SERVICE = \"psfu\"\n",
    "\n",
    "EXPO_PATH   = r\"C:\\Users\\ry16123\\Downloads\\EXPOSICION-new 1.xlsx\"\n",
    "EXPO_SHEET  = None  # <- auto-detección de hoja (dejalo None). Si querés fijar, poné el nombre exacto.\n",
    "\n",
    "# ================== QUERIES (según tu M) ==================\n",
    "SQL_FDP_DINA = \"\"\"\n",
    "SELECT DBU_FIC_ORG_ESTRUCTURAL.NOMBRE_CORTO,\n",
    "       DBU_FIC_ORG_ESTRUCTURAL.NIVEL_1,\n",
    "       DBU_FIC_ORG_ESTRUCTURAL.NIVEL_2,\n",
    "       DBU_FIC_ORG_ESTRUCTURAL.NIVEL_3,\n",
    "       DBU_FIC_ORG_ESTRUCTURAL.NIVEL_4,\n",
    "       DBU_FIC_ORG_ESTRUCTURAL.NIVEL_5,\n",
    "       FIC_ULTIMO_DINAMOMETRO.FECHA_HORA,\n",
    "       FIC_ULTIMO_DINAMOMETRO.REALIZADO_POR,\n",
    "       FIC_ULTIMO_DINAMOMETRO.AIB_MARCA_Y_DESC_API,\n",
    "       FIC_ULTIMO_DINAMOMETRO.AIBRR_TORQUE_MAXIMO_REDUCTOR,\n",
    "       FIC_ULTIMO_DINAMOMETRO.AIBEB_TORQUE_MAXIMO_REDUCTOR,\n",
    "       FIC_ULTIMO_DINAMOMETRO.AIBRR_TORQUE_DISPONIBLE,\n",
    "       FIC_ULTIMO_DINAMOMETRO.BBA_LLENADO_DE_BOMBA/100 AS LLENAD_BOMBA_PCT,\n",
    "       FIC_ULTIMO_DINAMOMETRO.AIBRE_SOLICITACION_DE_ESTRUCT/100 AS ESTRUCTURA_PCT,\n",
    "       VTOW_WELL_LAST_CONTROL_DET.PROD_OIL,\n",
    "       VTOW_WELL_LAST_CONTROL_DET.PROD_GAS,\n",
    "       VTOW_WELL_LAST_CONTROL_DET.PROD_WAT,\n",
    "       FIC_ULTIMO_DINAMOMETRO.AIB_GPM,\n",
    "       FIC_ULTIMO_DINAMOMETRO.AIB_DIAMETRO_POLEA_REDUCTOR,\n",
    "       FIC_ULTIMO_DINAMOMETRO.MOTOR_DIAMETRO_POLEA\n",
    "FROM DISC_ADMINS.DBU_FIC_ORG_ESTRUCTURAL DBU_FIC_ORG_ESTRUCTURAL,\n",
    "     DISC_ADMINS.VTOW_WELL_LAST_CONTROL_DET VTOW_WELL_LAST_CONTROL_DET,\n",
    "     DISC_ADMINS.FIC_ULTIMO_DINAMOMETRO FIC_ULTIMO_DINAMOMETRO\n",
    "WHERE ((VTOW_WELL_LAST_CONTROL_DET.COMP_SK = DBU_FIC_ORG_ESTRUCTURAL.COMP_SK)\n",
    "       AND (FIC_ULTIMO_DINAMOMETRO.COMP_SK = DBU_FIC_ORG_ESTRUCTURAL.COMP_SK))\n",
    "  AND (DBU_FIC_ORG_ESTRUCTURAL.ESTADO = 'Produciendo')\n",
    "  AND ((FIC_ULTIMO_DINAMOMETRO.AIBRE_SOLICITACION_DE_ESTRUCT/100) <> 0)\n",
    "  AND ((FIC_ULTIMO_DINAMOMETRO.BBA_LLENADO_DE_BOMBA/100) <> 0)\n",
    "  AND (DBU_FIC_ORG_ESTRUCTURAL.MET_PROD = 'Bombeo Mecánico')\n",
    "\"\"\"\n",
    "\n",
    "# Consulta \"Producción\" de tu M: UPPER(NIVEL_4), SUM(PROD_OIL) agrupado por NIVEL_4\n",
    "SQL_PRODUCCION = \"\"\"\n",
    "SELECT UPPER(DBU_FIC_ORG_ESTRUCTURAL.NIVEL_4) AS NIVEL_4,\n",
    "       SUM(VTOW_WELL_LAST_CONTROL_DET.PROD_OIL) AS PROD_OIL_1\n",
    "FROM DISC_ADMINS.DBU_FIC_ORG_ESTRUCTURAL DBU_FIC_ORG_ESTRUCTURAL,\n",
    "     DISC_ADMINS.VTOW_WELL_LAST_CONTROL_DET VTOW_WELL_LAST_CONTROL_DET,\n",
    "     DISC_ADMINS.FIC_ULTIMO_DINAMOMETRO FIC_ULTIMO_DINAMOMETRO\n",
    "WHERE ((VTOW_WELL_LAST_CONTROL_DET.COMP_SK = DBU_FIC_ORG_ESTRUCTURAL.COMP_SK)\n",
    "       AND (FIC_ULTIMO_DINAMOMETRO.COMP_SK = DBU_FIC_ORG_ESTRUCTURAL.COMP_SK))\n",
    "  AND (DBU_FIC_ORG_ESTRUCTURAL.ESTADO = 'Produciendo')\n",
    "  AND ((FIC_ULTIMO_DINAMOMETRO.AIBRE_SOLICITACION_DE_ESTRUCT/100) <> 0)\n",
    "  AND ((FIC_ULTIMO_DINAMOMETRO.BBA_LLENADO_DE_BOMBA/100) <> 0)\n",
    "  AND (DBU_FIC_ORG_ESTRUCTURAL.MET_PROD = 'Bombeo Mecánico')\n",
    "GROUP BY UPPER(DBU_FIC_ORG_ESTRUCTURAL.NIVEL_4)\n",
    "\"\"\"\n",
    "\n",
    "# ================== CONEXIÓN ORACLE (cx_Oracle) ==================\n",
    "def get_connection():\n",
    "    dsn = cx_Oracle.makedsn(ORA_HOST, ORA_PORT, service_name=ORA_SERVICE)\n",
    "    return cx_Oracle.connect(ORA_USER, ORA_PASS, dsn, encoding=\"UTF-8\")\n",
    "\n",
    "def read_sql_df(conn, sql: str) -> pd.DataFrame:\n",
    "    # pandas muestra un warning (recomienda SQLAlchemy), pero funciona igual con cx_Oracle.\n",
    "    return pd.read_sql(sql, con=conn)\n",
    "\n",
    "# ================== EXPO (auto-detección de hoja) ==================\n",
    "def load_exposicion(path: str, sheet: str | None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Carga el Excel de EXPOSICIÓN aunque la hoja no se llame 'EXPOSICION'.\n",
    "    Estrategia:\n",
    "      1) Si sheet está definido y existe -> lo usa.\n",
    "      2) Prueba nombres comunes.\n",
    "      3) Recorre todas las hojas y elige la que tenga la columna 'Denominacion API'.\n",
    "    \"\"\"\n",
    "    xls = pd.ExcelFile(path, engine=\"openpyxl\")\n",
    "    print(f\"Hojas encontradas en '{path}': {xls.sheet_names}\")\n",
    "\n",
    "    # 1) Hoja indicada por el usuario\n",
    "    if sheet and sheet in xls.sheet_names:\n",
    "        df = pd.read_excel(xls, sheet_name=sheet)\n",
    "    else:\n",
    "        # 2) Intentar nombres típicos\n",
    "        candidatos = [\"EXPOSICION\", \"EXPOSICIÓN\", \"Exposicion\", \"Hoja1\", \"Sheet1\"]\n",
    "        df = None\n",
    "        for s in candidatos:\n",
    "            if s in xls.sheet_names:\n",
    "                df = pd.read_excel(xls, sheet_name=s)\n",
    "                break\n",
    "        # 3) Buscar por columna 'Denominacion API'\n",
    "        if df is None:\n",
    "            for s in xls.sheet_names:\n",
    "                tmp = pd.read_excel(xls, sheet_name=s)\n",
    "                cols_norm = [str(c).strip().lower() for c in tmp.columns]\n",
    "                if \"denominacion api\" in cols_norm:\n",
    "                    df = tmp\n",
    "                    print(f\"Usando hoja detectada por columna: {s}\")\n",
    "                    break\n",
    "        if df is None:\n",
    "            raise ValueError(\n",
    "                f\"No pude encontrar la hoja de EXPOSICIÓN. \"\n",
    "                f\"Indicá EXPO_SHEET con el nombre exacto. Hojas: {xls.sheet_names}\"\n",
    "            )\n",
    "\n",
    "    # Quitar 'Cantidad' si existe\n",
    "    if \"Cantidad\" in df.columns:\n",
    "        df = df.drop(columns=[\"Cantidad\"])\n",
    "\n",
    "    # Normalizar columna clave (Denominacion API)\n",
    "    col_api = None\n",
    "    for c in df.columns:\n",
    "        if str(c).strip().lower() == \"denominacion api\":\n",
    "            col_api = c\n",
    "            break\n",
    "    if col_api is None:\n",
    "        raise ValueError(\"No encuentro la columna 'Denominacion API' en la hoja seleccionada.\")\n",
    "\n",
    "    df[col_api] = df[col_api].astype(str).str.upper().str.strip()\n",
    "    df = df.rename(columns={col_api: \"Denominacion API\"})\n",
    "\n",
    "    # Asegurar resto de columnas esperadas\n",
    "    for c in [\"Marca\", \"Tipo\", \"Torque max\", \"SENSIBILIDAD CR\", \"SENSIBILIDAD EST\"]:\n",
    "        if c not in df.columns:\n",
    "            df[c] = np.nan\n",
    "\n",
    "    return df\n",
    "\n",
    "# ================== FDP_DINA y Producción ==================\n",
    "def load_fdp(conn) -> pd.DataFrame:\n",
    "    df = read_sql_df(conn, SQL_FDP_DINA)\n",
    "    # Text.Upper(NIVEL_4)\n",
    "    df[\"NIVEL_4\"] = df[\"NIVEL_4\"].astype(str).str.upper()\n",
    "    return df\n",
    "\n",
    "def load_produccion(conn) -> pd.DataFrame:\n",
    "    return read_sql_df(conn, SQL_PRODUCCION)  # columnas: NIVEL_4, PROD_OIL_1\n",
    "\n",
    "# ================== Transformaciones equivalentes a Power Query ==================\n",
    "def join_expo(df_fdp: pd.DataFrame, expo: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df_fdp.copy()\n",
    "    df[\"__API_KEY__\"] = df[\"AIB_MARCA_Y_DESC_API\"].astype(str).str.upper().str.strip()\n",
    "    ex = expo.copy()\n",
    "    ex[\"__API_KEY__\"] = ex[\"Denominacion API\"]\n",
    "\n",
    "    df = df.merge(\n",
    "        ex[[\"__API_KEY__\",\"Denominacion API\",\"Marca\",\"Tipo\",\"Torque max\",\"SENSIBILIDAD CR\",\"SENSIBILIDAD EST\"]],\n",
    "        how=\"left\", on=\"__API_KEY__\"\n",
    "    ).drop(columns=[\"__API_KEY__\"])\n",
    "\n",
    "    # Prefijo EXPOSICIÓN.\n",
    "    df = df.rename(columns={\n",
    "        \"Denominacion API\": \"EXPOSICIÓN.Denominacion API\",\n",
    "        \"Marca\": \"EXPOSICIÓN.Marca\",\n",
    "        \"Tipo\": \"EXPOSICIÓN.Tipo\",\n",
    "        \"Torque max\": \"EXPOSICIÓN.Torque max\",\n",
    "        \"SENSIBILIDAD CR\": \"EXPOSICIÓN.SENSIBILIDAD CR\",\n",
    "        \"SENSIBILIDAD EST\": \"EXPOSICIÓN.SENSIBILIDAD EST\",\n",
    "    })\n",
    "\n",
    "    # ID = NOMBRE_CORTO & NIVEL_5 → filtrar y deduplicar\n",
    "    df[\"ID_tmp\"] = (df[\"NOMBRE_CORTO\"].fillna(\"\").astype(str) + df[\"NIVEL_5\"].fillna(\"\").astype(str))\n",
    "    df = df[(df[\"ID_tmp\"]!=\"\") & df[\"NOMBRE_CORTO\"].notna()]\n",
    "    df = df.drop_duplicates(subset=[\"ID_tmp\"]).drop(columns=[\"ID_tmp\"])\n",
    "\n",
    "    # Reemplazos de nulos por 0 en torques\n",
    "    for c in [\"AIBRR_TORQUE_MAXIMO_REDUCTOR\",\"AIBEB_TORQUE_MAXIMO_REDUCTOR\"]:\n",
    "        df[c] = df[c].fillna(0)\n",
    "\n",
    "    # Reemplazo Tierra del Fuego → Chubut\n",
    "    df[\"NIVEL_2\"] = df[\"NIVEL_2\"].replace({\"Tierra del Fuego\": \"Chubut\"})\n",
    "\n",
    "    return df\n",
    "\n",
    "def attach_produccion(df: pd.DataFrame, prod: pd.DataFrame) -> pd.DataFrame:\n",
    "    return df.merge(prod, how=\"left\", on=\"NIVEL_4\")\n",
    "\n",
    "# ================== Medidas/derivadas (clon DAX) ==================\n",
    "def add_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    d = df.copy()\n",
    "\n",
    "    # Ratio_Produccion\n",
    "    d[\"PROD_OIL_1\"] = d[\"PROD_OIL_1\"].replace({0: np.nan})\n",
    "    d[\"Ratio_Produccion\"] = d[\"PROD_OIL\"] / d[\"PROD_OIL_1\"]\n",
    "\n",
    "    # CONSECUENCIA (texto por cortes)\n",
    "    def seg_consecuencia(x):\n",
    "        if pd.isna(x): return \"0 - 0,0025\"\n",
    "        if x >= 0.0065: return \"Mayor a 0,0065\"\n",
    "        if x >= 0.0025: return \"0,0025 - 0,0065\"\n",
    "        return \"0 - 0,0025\"\n",
    "    d[\"f_CONSECUENCIA\"] = d[\"Ratio_Produccion\"].apply(seg_consecuencia)\n",
    "    d[\"CONSECUENCIA\"]   = d[\"f_CONSECUENCIA\"]\n",
    "\n",
    "    # TORQUE_BAL[%] y TORQUE[%]\n",
    "    d[\"TORQUE_BAL[%]\"] = d[\"AIBEB_TORQUE_MAXIMO_REDUCTOR\"] / d[\"AIBRR_TORQUE_DISPONIBLE\"]\n",
    "    d[\"TORQUE[%]\"]     = d[\"AIBRR_TORQUE_MAXIMO_REDUCTOR\"] / d[\"AIBRR_TORQUE_DISPONIBLE\"]\n",
    "\n",
    "    # Factores y EXIGENCIA (según tu M)\n",
    "    d[\"f_CR\"]  = np.select([d[\"TORQUE[%]\"] > 1, d[\"TORQUE[%]\"] >= 0.85], [10, 3.7], default=0)\n",
    "    d[\"f_CR3\"] = d[\"EXPOSICIÓN.SENSIBILIDAD CR\"].map({\"ALTA\":3, \"MEDIA\":1.5}).fillna(1.0)\n",
    "    d[\"CAJA REDUCTORA\"]  = d[\"f_CR\"] * d[\"f_CR3\"]\n",
    "    d[\"CAJA REDUCTORA_\"] = np.where(d[\"TORQUE_BAL[%]\"]>1, 1.5*d[\"CAJA REDUCTORA\"], d[\"CAJA REDUCTORA\"])\n",
    "\n",
    "    d[\"f_estr\"]  = np.select([d[\"ESTRUCTURA_PCT\"] >= 0.95, d[\"ESTRUCTURA_PCT\"] > 0.85], [20,5], default=1)\n",
    "    d[\"f_estr3\"] = d[\"EXPOSICIÓN.SENSIBILIDAD EST\"].map({\"ALTA\":3,\"MEDIA\":1.5}).fillna(1.0)\n",
    "    d[\"ESTRUCTURA\"] = d[\"f_estr\"] * d[\"f_estr3\"]\n",
    "\n",
    "    d[\"f_GPM\"]  = np.select([d[\"AIB_GPM\"] > 9, d[\"AIB_GPM\"] > 7], [3, 1.5], default=1)\n",
    "    d[\"f_GPM2\"] = np.where(d[\"LLENAD_BOMBA_PCT\"] < 0.75, d[\"f_GPM\"]*2, d[\"f_GPM\"])\n",
    "\n",
    "    d[\"EXIGENCIA\"]   = d[\"f_GPM2\"] * d[\"ESTRUCTURA\"] + d[\"CAJA REDUCTORA_\"]\n",
    "    d[\"F_EXIGENCIA\"] = np.select([d[\"EXIGENCIA\"]>10, d[\"EXIGENCIA\"]>4], [\"A\",\"M\"], default=\"B\")\n",
    "\n",
    "    # CRITICIDAD (reglas de tu M)\n",
    "    def crit(row):\n",
    "        rp = row[\"Ratio_Produccion\"]; fe = row[\"F_EXIGENCIA\"]\n",
    "        if pd.isna(rp) or pd.isna(fe): return \"NORMAL\"\n",
    "        if rp >= 0.0065 and fe == \"A\": return \"CRITICO\"\n",
    "        if rp >= 0.0025 and fe == \"A\": return \"ALERTA\"\n",
    "        if rp <  0.0025 and fe == \"A\": return \"ALERTA\"\n",
    "        if rp >= 0.0065 and fe == \"M\": return \"ALERTA\"\n",
    "        if rp >= 0.0025 and fe == \"M\": return \"ALERTA\"\n",
    "        if rp <  0.0025 and fe == \"M\": return \"ALERTA\"\n",
    "        return \"NORMAL\"\n",
    "    d[\"CRITICIDAD\"] = d.apply(crit, axis=1).astype(str)\n",
    "    d[\"ref_criticidad\"] = d[\"CRITICIDAD\"].map({\"CRITICO\":1, \"ALERTA\":2, \"NORMAL\":3}).astype(\"Int64\")\n",
    "\n",
    "    # Color Detalle CRITICIDAD (DAX)\n",
    "    def color_detalle(rp, fe):\n",
    "        if pd.isna(rp) or pd.isna(fe): return \"\"\n",
    "        if rp >= 0.0065 and fe == \"A\": return \"RED\"\n",
    "        if rp >= 0.0025 and fe == \"A\": return \"ORANGE\"\n",
    "        if rp <  0.0025 and fe == \"A\": return \"ORANGE\"\n",
    "        if rp >= 0.0065 and fe == \"M\": return \"YELLOW\"\n",
    "        if rp >= 0.0025 and fe == \"M\": return \"YELLOW\"\n",
    "        if rp <  0.0025 and fe == \"M\": return \"YELLOW\"\n",
    "        if rp >= 0.0065 and fe == \"B\": return \"GREEN\"\n",
    "        if rp >= 0.0025 and fe == \"B\": return \"GREEN\"\n",
    "        if rp <  0.0025 and fe == \"B\": return \"GREEN\"\n",
    "        return \"\"\n",
    "    d[\"Color Detalle CRITICIDAD\"] = np.vectorize(color_detalle)(d[\"Ratio_Produccion\"], d[\"F_EXIGENCIA\"])\n",
    "\n",
    "    # criticidad color / texto (DAX)\n",
    "    d[\"criticidad color\"] = d[\"CRITICIDAD\"].map({\"CRITICO\":\"Red\", \"ALERTA\":\"YELLOW\", \"NORMAL\":\"Green\"}).fillna(\"White\")\n",
    "    d[\"criticidad texto\"] = d[\"CRITICIDAD\"].map({\"CRITICO\":\"white\", \"ALERTA\":\"black\", \"NORMAL\":\"white\"}).fillna(\"White\")\n",
    "\n",
    "    # DETALLE CRITICIDAD (DAX)\n",
    "    def detalle_crit(rp, fe):\n",
    "        if pd.isna(rp) or pd.isna(fe): return \"NORMAL\"\n",
    "        if rp >= 0.0065 and fe == \"A\": return \"AA\"\n",
    "        if rp >= 0.0025 and fe == \"A\": return \"AM\"\n",
    "        if rp <  0.0025 and fe == \"A\": return \"AB\"\n",
    "        if rp >= 0.0065 and fe == \"M\": return \"MA\"\n",
    "        if rp >= 0.0025 and fe == \"M\": return \"MM\"\n",
    "        if rp <  0.0025 and fe == \"M\": return \"MB\"\n",
    "        if rp >= 0.0065 and fe == \"B\": return \"BA\"\n",
    "        if rp >= 0.0025 and fe == \"B\": return \"BM\"\n",
    "        if rp <  0.0025 and fe == \"B\": return \"BB\"\n",
    "        return \"NORMAL\"\n",
    "    d[\"DETALLE CRITICIDAD\"] = np.vectorize(detalle_crit)(d[\"Ratio_Produccion\"], d[\"F_EXIGENCIA\"])\n",
    "\n",
    "    # Días sin dina = DATEDIFF(MAX(FECHA_HORA), TODAY(), DAY) → MAX por pozo\n",
    "    d[\"FECHA_HORA\"] = pd.to_datetime(d[\"FECHA_HORA\"], errors=\"coerce\")\n",
    "    max_por_pozo = d.groupby(\"NOMBRE_CORTO\", dropna=False)[\"FECHA_HORA\"].max().rename(\"FECHA_MAX\").reset_index()\n",
    "    d = d.merge(max_por_pozo, on=\"NOMBRE_CORTO\", how=\"left\")\n",
    "    d[\"Días sin dina\"] = (pd.Timestamp.today().normalize() - d[\"FECHA_MAX\"].dt.normalize()).dt.days\n",
    "\n",
    "    # Columnas calculadas dentro de FDP_DINA\n",
    "    d[\"Correlativo Exigencia\"] = d[\"F_EXIGENCIA\"].map({\"A\":\"10 +\", \"M\":\"04-10\", \"B\":\"0-04\"}).fillna(\"\")\n",
    "    d[\"ID\"] = d[\"f_CONSECUENCIA\"].fillna(\"\").astype(str) + d[\"Correlativo Exigencia\"].fillna(\"\").astype(str)\n",
    "\n",
    "    # Filtro final igual que M\n",
    "    d = d[(d[\"NOMBRE_CORTO\"].notna()) & (d[\"NOMBRE_CORTO\"].astype(str)!=\"\")]\n",
    "\n",
    "    return d\n",
    "\n",
    "# ================== PIPELINE ==================\n",
    "def main():\n",
    "    print(\"Conectando a Oracle (cx_Oracle)…\")\n",
    "    with get_connection() as conn:\n",
    "        print(\"Leyendo FDP_DINA…\")\n",
    "        fdp = load_fdp(conn)\n",
    "\n",
    "        print(\"Leyendo Producción (sum PROD_OIL por NIVEL_4)…\")\n",
    "        prod = load_produccion(conn)\n",
    "\n",
    "    print(\"Leyendo Excel EXPOSICIÓN…\")\n",
    "    expo = load_exposicion(EXPO_PATH, EXPO_SHEET)\n",
    "\n",
    "    print(\"Aplicando joins/limpieza (EXPOSICIÓN)…\")\n",
    "    fdp = join_expo(fdp, expo)\n",
    "\n",
    "    print(\"Adjuntando Producción…\")\n",
    "    fdp = attach_produccion(fdp, prod)\n",
    "\n",
    "    print(\"Calculando métricas / criticidad…\")\n",
    "    final = add_features(fdp)\n",
    "\n",
    "    out = Path(\"Matriz_AIB_export.xlsx\")\n",
    "    final.to_excel(out, index=False)\n",
    "    print(f\"✅ Exportado {len(final):,} filas → {out.absolute()}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7ce0e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conectando a Oracle (cx_Oracle)…\n",
      "Leyendo FDP_DINA…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ry16123\\AppData\\Local\\Temp\\ipykernel_41768\\2025247171.py:105: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  return pd.read_sql(sql, con=conn)  # warning inofensivo\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leyendo Producción…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ry16123\\AppData\\Local\\Temp\\ipykernel_41768\\2025247171.py:105: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  return pd.read_sql(sql, con=conn)  # warning inofensivo\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leyendo ACCIONES…\n",
      "Leyendo Excel EXPOSICIÓN…\n",
      "Hojas encontradas en 'C:\\Users\\ry16123\\Downloads\\EXPOSICION-new 1.xlsx': ['Hoja5', 'EXPOSICIÓN', 'Hoja1', 'Hoja2', 'IDP', 'Hoja3', 'OBSERVACIONES']\n",
      "Aplicando joins/limpieza (EXPOSICIÓN)…\n",
      "Adjuntando Producción…\n",
      "Calculando métricas / criticidad…\n",
      "✅ Exportado 3,767 filas (MATRIZ) + 17,710 filas (ACCIONES) → C:\\Users\\ry16123\\Matriz_AIB_export.xlsx\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Matriz de Criticidad de AIBs (fuera de Power BI) - cx_Oracle\n",
    "# Hoja 1: columnas solicitadas de FDP_DINA transformada\n",
    "# Hoja 2: consulta ACCIONES\n",
    "\n",
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cx_Oracle\n",
    "\n",
    "# ================== CREDENCIALES / RUTAS ==================\n",
    "ORA_USER    = \"RY16123\"\n",
    "ORA_PASS    = \"Luciano284\"\n",
    "ORA_HOST    = \"slplpgmoora03\"\n",
    "ORA_PORT    = 1527\n",
    "ORA_SERVICE = \"psfu\"\n",
    "\n",
    "EXPO_PATH   = r\"C:\\Users\\ry16123\\Downloads\\EXPOSICION-new 1.xlsx\"\n",
    "EXPO_SHEET  = None  # auto-detección de hoja\n",
    "\n",
    "# ================== QUERIES ==================\n",
    "SQL_FDP_DINA = \"\"\"\n",
    "SELECT DBU_FIC_ORG_ESTRUCTURAL.NOMBRE_CORTO,\n",
    "       DBU_FIC_ORG_ESTRUCTURAL.NIVEL_1,\n",
    "       DBU_FIC_ORG_ESTRUCTURAL.NIVEL_2,\n",
    "       DBU_FIC_ORG_ESTRUCTURAL.NIVEL_3,\n",
    "       DBU_FIC_ORG_ESTRUCTURAL.NIVEL_4,\n",
    "       DBU_FIC_ORG_ESTRUCTURAL.NIVEL_5,\n",
    "       FIC_ULTIMO_DINAMOMETRO.FECHA_HORA,\n",
    "       FIC_ULTIMO_DINAMOMETRO.REALIZADO_POR,\n",
    "       FIC_ULTIMO_DINAMOMETRO.AIB_MARCA_Y_DESC_API,\n",
    "       FIC_ULTIMO_DINAMOMETRO.AIBRR_TORQUE_MAXIMO_REDUCTOR,\n",
    "       FIC_ULTIMO_DINAMOMETRO.AIBEB_TORQUE_MAXIMO_REDUCTOR,\n",
    "       FIC_ULTIMO_DINAMOMETRO.AIBRR_TORQUE_DISPONIBLE,\n",
    "       FIC_ULTIMO_DINAMOMETRO.BBA_LLENADO_DE_BOMBA/100 AS LLENAD_BOMBA_PCT,\n",
    "       FIC_ULTIMO_DINAMOMETRO.AIBRE_SOLICITACION_DE_ESTRUCT/100 AS ESTRUCTURA_PCT,\n",
    "       VTOW_WELL_LAST_CONTROL_DET.PROD_OIL,\n",
    "       VTOW_WELL_LAST_CONTROL_DET.PROD_GAS,\n",
    "       VTOW_WELL_LAST_CONTROL_DET.PROD_WAT,\n",
    "       FIC_ULTIMO_DINAMOMETRO.AIB_GPM,\n",
    "       FIC_ULTIMO_DINAMOMETRO.AIB_DIAMETRO_POLEA_REDUCTOR,\n",
    "       FIC_ULTIMO_DINAMOMETRO.MOTOR_DIAMETRO_POLEA\n",
    "FROM DISC_ADMINS.DBU_FIC_ORG_ESTRUCTURAL DBU_FIC_ORG_ESTRUCTURAL,\n",
    "     DISC_ADMINS.VTOW_WELL_LAST_CONTROL_DET VTOW_WELL_LAST_CONTROL_DET,\n",
    "     DISC_ADMINS.FIC_ULTIMO_DINAMOMETRO FIC_ULTIMO_DINAMOMETRO\n",
    "WHERE ((VTOW_WELL_LAST_CONTROL_DET.COMP_SK = DBU_FIC_ORG_ESTRUCTURAL.COMP_SK)\n",
    "       AND (FIC_ULTIMO_DINAMOMETRO.COMP_SK = DBU_FIC_ORG_ESTRUCTURAL.COMP_SK))\n",
    "  AND (DBU_FIC_ORG_ESTRUCTURAL.ESTADO = 'Produciendo')\n",
    "  AND ((FIC_ULTIMO_DINAMOMETRO.AIBRE_SOLICITACION_DE_ESTRUCT/100) <> 0)\n",
    "  AND ((FIC_ULTIMO_DINAMOMETRO.BBA_LLENADO_DE_BOMBA/100) <> 0)\n",
    "  AND (DBU_FIC_ORG_ESTRUCTURAL.MET_PROD = 'Bombeo Mecánico')\n",
    "\"\"\"\n",
    "\n",
    "SQL_PRODUCCION = \"\"\"\n",
    "SELECT UPPER(DBU_FIC_ORG_ESTRUCTURAL.NIVEL_4) AS NIVEL_4,\n",
    "       SUM(VTOW_WELL_LAST_CONTROL_DET.PROD_OIL) AS PROD_OIL_1\n",
    "FROM DISC_ADMINS.DBU_FIC_ORG_ESTRUCTURAL DBU_FIC_ORG_ESTRUCTURAL,\n",
    "     DISC_ADMINS.VTOW_WELL_LAST_CONTROL_DET VTOW_WELL_LAST_CONTROL_DET,\n",
    "     DISC_ADMINS.FIC_ULTIMO_DINAMOMETRO FIC_ULTIMO_DINAMOMETRO\n",
    "WHERE ((VTOW_WELL_LAST_CONTROL_DET.COMP_SK = DBU_FIC_ORG_ESTRUCTURAL.COMP_SK)\n",
    "       AND (FIC_ULTIMO_DINAMOMETRO.COMP_SK = DBU_FIC_ORG_ESTRUCTURAL.COMP_SK))\n",
    "  AND (DBU_FIC_ORG_ESTRUCTURAL.ESTADO = 'Produciendo')\n",
    "  AND ((FIC_ULTIMO_DINAMOMETRO.AIBRE_SOLICITACION_DE_ESTRUCT/100) <> 0)\n",
    "  AND ((FIC_ULTIMO_DINAMOMETRO.BBA_LLENADO_DE_BOMBA/100) <> 0)\n",
    "  AND (DBU_FIC_ORG_ESTRUCTURAL.MET_PROD = 'Bombeo Mecánico')\n",
    "GROUP BY UPPER(DBU_FIC_ORG_ESTRUCTURAL.NIVEL_4)\n",
    "\"\"\"\n",
    "\n",
    "SQL_ACCIONES = \"\"\"\n",
    "SELECT DISTINCT\n",
    "  DBU_FIC_ORG_ESTRUCTURAL.NIVEL_3,\n",
    "  FIC_ACCIONES.ACTIVIDAD,\n",
    "  FIC_ACCIONES.ESTADO,\n",
    "  FIC_ACCIONES.\"ESTADO AUTORIZACIÓN\",\n",
    "  FIC_ACCIONES.FECHAACCION,\n",
    "  FIC_ACCIONES.\"FECHA AUTORIZACION\",\n",
    "  FIC_ACCIONES.FECHAREALIZACION,\n",
    "  FIC_ACCIONES.JUSTIFICACION,\n",
    "  FIC_ACCIONES.OBJETIVO,\n",
    "  FIC_ACCIONES.OBSERVACION,\n",
    "  FIC_ACCIONES.ORIGEN,\n",
    "  FIC_ACCIONES.\"INCREMENTO BRUTA\",\n",
    "  FIC_ACCIONES.\"INCREMENTO PETRÓLEO\",\n",
    "  FIC_ACCIONES.RECURSO,\n",
    "  FIC_ACCIONES.SUBACTIVIDAD,\n",
    "  FIC_ACCIONES.\"USUARIO AUTORIZANTE\",\n",
    "  FIC_ACCIONES.\"USUARIO CREADOR\",\n",
    "  DBU_FIC_ORG_ESTRUCTURAL.NOMBRE_POZO,\n",
    "  DBU_FIC_ORG_ESTRUCTURAL.NOMBRE_CORTO_POZO\n",
    "FROM DISC_ADMINS.DBU_FIC_ORG_ESTRUCTURAL DBU_FIC_ORG_ESTRUCTURAL,\n",
    "     DISC_ADMINS.FIC_ACCIONES FIC_ACCIONES\n",
    "WHERE (FIC_ACCIONES.CLAVEPOZO = DBU_FIC_ORG_ESTRUCTURAL.COMP_SK)\n",
    "  AND (DBU_FIC_ORG_ESTRUCTURAL.COMP_SK = FIC_ACCIONES.CLAVEPOZO)\n",
    "  AND (FIC_ACCIONES.FECHAACCION >= TO_DATE('20230101000000','YYYYMMDDHH24MISS'))\n",
    "  AND (DBU_FIC_ORG_ESTRUCTURAL.NIVEL_3 IN ('Las Heras CG - Canadon Escondida','Los Perales','El Guadal','Seco Leon - Pico Truncado'))\n",
    "\"\"\"\n",
    "\n",
    "# ================== CONEXIÓN ORACLE (cx_Oracle) ==================\n",
    "def get_connection():\n",
    "    dsn = cx_Oracle.makedsn(ORA_HOST, ORA_PORT, service_name=ORA_SERVICE)\n",
    "    return cx_Oracle.connect(ORA_USER, ORA_PASS, dsn, encoding=\"UTF-8\")\n",
    "\n",
    "def read_sql_df(conn, sql: str) -> pd.DataFrame:\n",
    "    return pd.read_sql(sql, con=conn)  # warning inofensivo\n",
    "\n",
    "# ================== EXPO (auto-detección de hoja) ==================\n",
    "def load_exposicion(path: str, sheet: str | None) -> pd.DataFrame:\n",
    "    xls = pd.ExcelFile(path, engine=\"openpyxl\")\n",
    "    print(f\"Hojas encontradas en '{path}': {xls.sheet_names}\")\n",
    "\n",
    "    if sheet and sheet in xls.sheet_names:\n",
    "        df = pd.read_excel(xls, sheet_name=sheet)\n",
    "    else:\n",
    "        candidatos = [\"EXPOSICION\", \"EXPOSICIÓN\", \"Exposicion\", \"Hoja1\", \"Sheet1\"]\n",
    "        df = None\n",
    "        for s in candidatos:\n",
    "            if s in xls.sheet_names:\n",
    "                df = pd.read_excel(xls, sheet_name=s)\n",
    "                break\n",
    "        if df is None:\n",
    "            for s in xls.sheet_names:\n",
    "                tmp = pd.read_excel(xls, sheet_name=s)\n",
    "                cols_norm = [str(c).strip().lower() for c in tmp.columns]\n",
    "                if \"denominacion api\" in cols_norm:\n",
    "                    df = tmp\n",
    "                    print(f\"Usando hoja detectada por columna: {s}\")\n",
    "                    break\n",
    "        if df is None:\n",
    "            raise ValueError(f\"No pude encontrar la hoja de EXPOSICIÓN. Hojas: {xls.sheet_names}\")\n",
    "\n",
    "    if \"Cantidad\" in df.columns:\n",
    "        df = df.drop(columns=[\"Cantidad\"])\n",
    "\n",
    "    # Normalizar columna clave\n",
    "    col_api = None\n",
    "    for c in df.columns:\n",
    "        if str(c).strip().lower() == \"denominacion api\":\n",
    "            col_api = c; break\n",
    "    if col_api is None:\n",
    "        raise ValueError(\"No encuentro la columna 'Denominacion API' en la hoja seleccionada.\")\n",
    "\n",
    "    df[col_api] = df[col_api].astype(str).str.upper().str.strip()\n",
    "    df = df.rename(columns={col_api: \"Denominacion API\"})\n",
    "\n",
    "    for c in [\"Marca\",\"Tipo\",\"Torque max\",\"SENSIBILIDAD CR\",\"SENSIBILIDAD EST\"]:\n",
    "        if c not in df.columns:\n",
    "            df[c] = np.nan\n",
    "    return df\n",
    "\n",
    "# ================== Cargas base ==================\n",
    "def load_fdp(conn) -> pd.DataFrame:\n",
    "    df = read_sql_df(conn, SQL_FDP_DINA)\n",
    "    df[\"NIVEL_4\"] = df[\"NIVEL_4\"].astype(str).str.upper()\n",
    "    return df\n",
    "\n",
    "def load_produccion(conn) -> pd.DataFrame:\n",
    "    return read_sql_df(conn, SQL_PRODUCCION)  # NIVEL_4, PROD_OIL_1\n",
    "\n",
    "def load_acciones(conn) -> pd.DataFrame:\n",
    "    df = read_sql_df(conn, SQL_ACCIONES)\n",
    "    # opcional: convertir fechas a datetime\n",
    "    for c in [\"FECHAACCION\",\"FECHAREALIZACION\",\"FECHA AUTORIZACION\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_datetime(df[c], errors=\"coerce\")\n",
    "    return df\n",
    "\n",
    "# ================== Transformaciones ==================\n",
    "def join_expo(df_fdp: pd.DataFrame, expo: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df_fdp.copy()\n",
    "    df[\"__API_KEY__\"] = df[\"AIB_MARCA_Y_DESC_API\"].astype(str).str.upper().str.strip()\n",
    "    ex = expo.copy()\n",
    "    ex[\"__API_KEY__\"] = ex[\"Denominacion API\"]\n",
    "\n",
    "    df = df.merge(\n",
    "        ex[[\"__API_KEY__\",\"Denominacion API\",\"Marca\",\"Tipo\",\"Torque max\",\"SENSIBILIDAD CR\",\"SENSIBILIDAD EST\"]],\n",
    "        how=\"left\", on=\"__API_KEY__\"\n",
    "    ).drop(columns=[\"__API_KEY__\"])\n",
    "\n",
    "    df = df.rename(columns={\n",
    "        \"Denominacion API\": \"EXPOSICIÓN.Denominacion API\",\n",
    "        \"Marca\": \"EXPOSICIÓN.Marca\",\n",
    "        \"Tipo\": \"EXPOSICIÓN.Tipo\",\n",
    "        \"Torque max\": \"EXPOSICIÓN.Torque max\",\n",
    "        \"SENSIBILIDAD CR\": \"EXPOSICIÓN.SENSIBILIDAD CR\",\n",
    "        \"SENSIBILIDAD EST\": \"EXPOSICIÓN.SENSIBILIDAD EST\",\n",
    "    })\n",
    "\n",
    "    df[\"ID_tmp\"] = (df[\"NOMBRE_CORTO\"].fillna(\"\").astype(str) + df[\"NIVEL_5\"].fillna(\"\").astype(str))\n",
    "    df = df[(df[\"ID_tmp\"]!=\"\") & df[\"NOMBRE_CORTO\"].notna()]\n",
    "    df = df.drop_duplicates(subset=[\"ID_tmp\"]).drop(columns=[\"ID_tmp\"])\n",
    "\n",
    "    for c in [\"AIBRR_TORQUE_MAXIMO_REDUCTOR\",\"AIBEB_TORQUE_MAXIMO_REDUCTOR\"]:\n",
    "        df[c] = df[c].fillna(0)\n",
    "\n",
    "    df[\"NIVEL_2\"] = df[\"NIVEL_2\"].replace({\"Tierra del Fuego\": \"Chubut\"})\n",
    "    return df\n",
    "\n",
    "def attach_produccion(df: pd.DataFrame, prod: pd.DataFrame) -> pd.DataFrame:\n",
    "    return df.merge(prod, how=\"left\", on=\"NIVEL_4\")\n",
    "\n",
    "def add_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    d = df.copy()\n",
    "    d[\"PROD_OIL_1\"] = d[\"PROD_OIL_1\"].replace({0: np.nan})\n",
    "    d[\"Ratio_Produccion\"] = d[\"PROD_OIL\"] / d[\"PROD_OIL_1\"]\n",
    "\n",
    "    def seg_consecuencia(x):\n",
    "        if pd.isna(x): return \"0 - 0,0025\"\n",
    "        if x >= 0.0065: return \"Mayor a 0,0065\"\n",
    "        if x >= 0.0025: return \"0,0025 - 0,0065\"\n",
    "        return \"0 - 0,0025\"\n",
    "    d[\"f_CONSECUENCIA\"] = d[\"Ratio_Produccion\"].apply(seg_consecuencia)\n",
    "    d[\"CONSECUENCIA\"] = d[\"f_CONSECUENCIA\"]\n",
    "\n",
    "    d[\"TORQUE_BAL[%]\"] = d[\"AIBEB_TORQUE_MAXIMO_REDUCTOR\"] / d[\"AIBRR_TORQUE_DISPONIBLE\"]\n",
    "    d[\"TORQUE[%]\"]     = d[\"AIBRR_TORQUE_MAXIMO_REDUCTOR\"] / d[\"AIBRR_TORQUE_DISPONIBLE\"]\n",
    "\n",
    "    d[\"f_CR\"]  = np.select([d[\"TORQUE[%]\"] > 1, d[\"TORQUE[%]\"] >= 0.85], [10, 3.7], default=0)\n",
    "    d[\"f_CR3\"] = d[\"EXPOSICIÓN.SENSIBILIDAD CR\"].map({\"ALTA\":3, \"MEDIA\":1.5}).fillna(1.0)\n",
    "    d[\"CAJA REDUCTORA\"]  = d[\"f_CR\"] * d[\"f_CR3\"]\n",
    "    d[\"CAJA REDUCTORA_\"] = np.where(d[\"TORQUE_BAL[%]\"]>1, 1.5*d[\"CAJA REDUCTORA\"], d[\"CAJA REDUCTORA\"])\n",
    "\n",
    "    d[\"f_estr\"]  = np.select([d[\"ESTRUCTURA_PCT\"] >= 0.95, d[\"ESTRUCTURA_PCT\"] > 0.85], [20,5], default=1)\n",
    "    d[\"f_estr3\"] = d[\"EXPOSICIÓN.SENSIBILIDAD EST\"].map({\"ALTA\":3,\"MEDIA\":1.5}).fillna(1.0)\n",
    "    d[\"ESTRUCTURA\"] = d[\"f_estr\"] * d[\"f_estr3\"]\n",
    "\n",
    "    d[\"f_GPM\"]  = np.select([d[\"AIB_GPM\"] > 9, d[\"AIB_GPM\"] > 7], [3, 1.5], default=1)\n",
    "    d[\"f_GPM2\"] = np.where(d[\"LLENAD_BOMBA_PCT\"] < 0.75, d[\"f_GPM\"]*2, d[\"f_GPM\"])\n",
    "\n",
    "    d[\"EXIGENCIA\"]   = d[\"f_GPM2\"] * d[\"ESTRUCTURA\"] + d[\"CAJA REDUCTORA_\"]\n",
    "    d[\"F_EXIGENCIA\"] = np.select([d[\"EXIGENCIA\"]>10, d[\"EXIGENCIA\"]>4], [\"A\",\"M\"], default=\"B\")\n",
    "\n",
    "    def crit(row):\n",
    "        rp = row[\"Ratio_Produccion\"]; fe = row[\"F_EXIGENCIA\"]\n",
    "        if pd.isna(rp) or pd.isna(fe): return \"NORMAL\"\n",
    "        if rp >= 0.0065 and fe == \"A\": return \"CRITICO\"\n",
    "        if rp >= 0.0025 and fe == \"A\": return \"ALERTA\"\n",
    "        if rp <  0.0025 and fe == \"A\": return \"ALERTA\"\n",
    "        if rp >= 0.0065 and fe == \"M\": return \"ALERTA\"\n",
    "        if rp >= 0.0025 and fe == \"M\": return \"ALERTA\"\n",
    "        if rp <  0.0025 and fe == \"M\": return \"ALERTA\"\n",
    "        return \"NORMAL\"\n",
    "    d[\"CRITICIDAD\"] = d.apply(crit, axis=1).astype(str)\n",
    "    d[\"ref_criticidad\"] = d[\"CRITICIDAD\"].map({\"CRITICO\":1, \"ALERTA\":2, \"NORMAL\":3}).astype(\"Int64\")\n",
    "\n",
    "    def color_detalle(rp, fe):\n",
    "        if pd.isna(rp) or pd.isna(fe): return \"\"\n",
    "        if rp >= 0.0065 and fe == \"A\": return \"RED\"\n",
    "        if rp >= 0.0025 and fe == \"A\": return \"ORANGE\"\n",
    "        if rp <  0.0025 and fe == \"A\": return \"ORANGE\"\n",
    "        if rp >= 0.0065 and fe == \"M\": return \"YELLOW\"\n",
    "        if rp >= 0.0025 and fe == \"M\": return \"YELLOW\"\n",
    "        if rp <  0.0025 and fe == \"M\": return \"YELLOW\"\n",
    "        if rp >= 0.0065 and fe == \"B\": return \"GREEN\"\n",
    "        if rp >= 0.0025 and fe == \"B\": return \"GREEN\"\n",
    "        if rp <  0.0025 and fe == \"B\": return \"GREEN\"\n",
    "        return \"\"\n",
    "    d[\"Color Detalle CRITICIDAD\"] = np.vectorize(color_detalle)(d[\"Ratio_Produccion\"], d[\"F_EXIGENCIA\"])\n",
    "\n",
    "    d[\"criticidad color\"] = d[\"CRITICIDAD\"].map({\"CRITICO\":\"Red\", \"ALERTA\":\"YELLOW\", \"NORMAL\":\"Green\"}).fillna(\"White\")\n",
    "    d[\"criticidad texto\"] = d[\"CRITICIDAD\"].map({\"CRITICO\":\"white\", \"ALERTA\":\"black\", \"NORMAL\":\"white\"}).fillna(\"White\")\n",
    "\n",
    "    def detalle_crit(rp, fe):\n",
    "        if pd.isna(rp) or pd.isna(fe): return \"NORMAL\"\n",
    "        if rp >= 0.0065 and fe == \"A\": return \"AA\"\n",
    "        if rp >= 0.0025 and fe == \"A\": return \"AM\"\n",
    "        if rp <  0.0025 and fe == \"A\": return \"AB\"\n",
    "        if rp >= 0.0065 and fe == \"M\": return \"MA\"\n",
    "        if rp >= 0.0025 and fe == \"M\": return \"MM\"\n",
    "        if rp <  0.0025 and fe == \"M\": return \"MB\"\n",
    "        if rp >= 0.0065 and fe == \"B\": return \"BA\"\n",
    "        if rp >= 0.0025 and fe == \"B\": return \"BM\"\n",
    "        if rp <  0.0025 and fe == \"B\": return \"BB\"\n",
    "        return \"NORMAL\"\n",
    "    d[\"DETALLE CRITICIDAD\"] = np.vectorize(detalle_crit)(d[\"Ratio_Produccion\"], d[\"F_EXIGENCIA\"])\n",
    "\n",
    "    d[\"FECHA_HORA\"] = pd.to_datetime(d[\"FECHA_HORA\"], errors=\"coerce\")\n",
    "    max_por_pozo = d.groupby(\"NOMBRE_CORTO\", dropna=False)[\"FECHA_HORA\"].max().rename(\"FECHA_MAX\").reset_index()\n",
    "    d = d.merge(max_por_pozo, on=\"NOMBRE_CORTO\", how=\"left\")\n",
    "    d[\"Días sin dina\"] = (pd.Timestamp.today().normalize() - d[\"FECHA_MAX\"].dt.normalize()).dt.days\n",
    "\n",
    "    d[\"Correlativo Exigencia\"] = d[\"F_EXIGENCIA\"].map({\"A\":\"10 +\", \"M\":\"04-10\", \"B\":\"0-04\"}).fillna(\"\")\n",
    "    d[\"ID\"] = d[\"f_CONSECUENCIA\"].fillna(\"\").astype(str) + d[\"Correlativo Exigencia\"].fillna(\"\").astype(str)\n",
    "\n",
    "    d = d[(d[\"NOMBRE_CORTO\"].notna()) & (d[\"NOMBRE_CORTO\"].astype(str)!=\"\")]\n",
    "\n",
    "    return d\n",
    "\n",
    "# ================== PIPELINE ==================\n",
    "def main():\n",
    "    print(\"Conectando a Oracle (cx_Oracle)…\")\n",
    "    with get_connection() as conn:\n",
    "        print(\"Leyendo FDP_DINA…\")\n",
    "        fdp = load_fdp(conn)\n",
    "\n",
    "        print(\"Leyendo Producción…\")\n",
    "        prod = load_produccion(conn)\n",
    "\n",
    "        print(\"Leyendo ACCIONES…\")\n",
    "        acciones = load_acciones(conn)\n",
    "\n",
    "    print(\"Leyendo Excel EXPOSICIÓN…\")\n",
    "    expo = load_exposicion(EXPO_PATH, EXPO_SHEET)\n",
    "\n",
    "    print(\"Aplicando joins/limpieza (EXPOSICIÓN)…\")\n",
    "    fdp = join_expo(fdp, expo)\n",
    "\n",
    "    print(\"Adjuntando Producción…\")\n",
    "    fdp = attach_produccion(fdp, prod)\n",
    "\n",
    "    print(\"Calculando métricas / criticidad…\")\n",
    "    final = add_features(fdp)\n",
    "\n",
    "    # -------- (1) Subset de columnas solicitadas --------\n",
    "    columnas_matriz = [\n",
    "        \"NOMBRE_CORTO\",\n",
    "        \"FECHA_HORA\",\n",
    "        \"AIB_MARCA_Y_DESC_API\",\n",
    "        \"PROD_OIL\",\n",
    "        \"PROD_GAS\",\n",
    "        \"PROD_WAT\",\n",
    "        \"AIB_GPM\",\n",
    "        \"MOTOR_DIAMETRO_POLEA\",\n",
    "        \"EXPOSICIÓN.Denominacion API\",\n",
    "        \"CRITICIDAD\",\n",
    "        \"Días sin dina\",\n",
    "    ]\n",
    "    faltantes = [c for c in columnas_matriz if c not in final.columns]\n",
    "    if faltantes:\n",
    "        raise KeyError(f\"Faltan columnas en la matriz final: {faltantes}\")\n",
    "    matriz_subset = final[columnas_matriz].copy()\n",
    "\n",
    "    # -------- (2) Exportar a Excel con 2 hojas --------\n",
    "    out = Path(\"Matriz_AIB_export.xlsx\")\n",
    "    with pd.ExcelWriter(out, engine=\"openpyxl\") as writer:\n",
    "        matriz_subset.to_excel(writer, sheet_name=\"MATRIZ\", index=False)\n",
    "        acciones.to_excel(writer, sheet_name=\"ACCIONES\", index=False)\n",
    "\n",
    "    print(f\"✅ Exportado {len(matriz_subset):,} filas (MATRIZ) + {len(acciones):,} filas (ACCIONES) → {out.absolute()}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c267d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conectando a Oracle (cx_Oracle)…\n",
      "Leyendo FDP_DINA…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ry16123\\AppData\\Local\\Temp\\ipykernel_33200\\669960166.py:107: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  return pd.read_sql(sql, con=conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leyendo Producción…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ry16123\\AppData\\Local\\Temp\\ipykernel_33200\\669960166.py:107: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  return pd.read_sql(sql, con=conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leyendo ACCIONES…\n",
      "Leyendo Excel EXPOSICIÓN…\n",
      "Hojas encontradas en 'C:\\Users\\ry16123\\Downloads\\EXPOSICION-new 1.xlsx': ['Hoja5', 'EXPOSICIÓN', 'Hoja1', 'Hoja2', 'IDP', 'Hoja3', 'OBSERVACIONES']\n",
      "Aplicando joins/limpieza (EXPOSICIÓN)…\n",
      "Adjuntando Producción…\n",
      "Calculando métricas / criticidad…\n",
      "✅ Exportado 3,766 filas (MATRIZ) + 17,715 filas (ACCIONES) → C:\\Users\\ry16123\\Matriz_AIB_export.xlsx\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Matriz de Criticidad de AIBs (fuera de Power BI) - cx_Oracle\n",
    "# Hoja 1: columnas solicitadas de FDP_DINA transformada\n",
    "# Hoja 2: consulta ACCIONES + columna ETIQUETA (clasificación por reglas + re-etiquetado)\n",
    "\n",
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "import re\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cx_Oracle\n",
    "\n",
    "# ================== CREDENCIALES / RUTAS ==================\n",
    "ORA_USER    = \"RY16123\"\n",
    "ORA_PASS    = \"Luciano284\"\n",
    "ORA_HOST    = \"slplpgmoora03\"\n",
    "ORA_PORT    = 1527\n",
    "ORA_SERVICE = \"psfu\"\n",
    "\n",
    "EXPO_PATH   = r\"C:\\Users\\ry16123\\Downloads\\EXPOSICION-new 1.xlsx\"\n",
    "EXPO_SHEET  = None  # auto-detección de hoja\n",
    "\n",
    "# ================== QUERIES ==================\n",
    "SQL_FDP_DINA = \"\"\"\n",
    "SELECT DBU_FIC_ORG_ESTRUCTURAL.NOMBRE_CORTO,\n",
    "       DBU_FIC_ORG_ESTRUCTURAL.NIVEL_1,\n",
    "       DBU_FIC_ORG_ESTRUCTURAL.NIVEL_2,\n",
    "       DBU_FIC_ORG_ESTRUCTURAL.NIVEL_3,\n",
    "       DBU_FIC_ORG_ESTRUCTURAL.NIVEL_4,\n",
    "       DBU_FIC_ORG_ESTRUCTURAL.NIVEL_5,\n",
    "       FIC_ULTIMO_DINAMOMETRO.FECHA_HORA,\n",
    "       FIC_ULTIMO_DINAMOMETRO.REALIZADO_POR,\n",
    "       FIC_ULTIMO_DINAMOMETRO.AIB_MARCA_Y_DESC_API,\n",
    "       FIC_ULTIMO_DINAMOMETRO.AIBRR_TORQUE_MAXIMO_REDUCTOR,\n",
    "       FIC_ULTIMO_DINAMOMETRO.AIBEB_TORQUE_MAXIMO_REDUCTOR,\n",
    "       FIC_ULTIMO_DINAMOMETRO.AIBRR_TORQUE_DISPONIBLE,\n",
    "       FIC_ULTIMO_DINAMOMETRO.BBA_LLENADO_DE_BOMBA/100 AS LLENAD_BOMBA_PCT,\n",
    "       FIC_ULTIMO_DINAMOMETRO.AIBRE_SOLICITACION_DE_ESTRUCT/100 AS ESTRUCTURA_PCT,\n",
    "       VTOW_WELL_LAST_CONTROL_DET.PROD_OIL,\n",
    "       VTOW_WELL_LAST_CONTROL_DET.PROD_GAS,\n",
    "       VTOW_WELL_LAST_CONTROL_DET.PROD_WAT,\n",
    "       FIC_ULTIMO_DINAMOMETRO.AIB_GPM,\n",
    "       FIC_ULTIMO_DINAMOMETRO.AIB_DIAMETRO_POLEA_REDUCTOR,\n",
    "       FIC_ULTIMO_DINAMOMETRO.MOTOR_DIAMETRO_POLEA\n",
    "FROM DISC_ADMINS.DBU_FIC_ORG_ESTRUCTURAL DBU_FIC_ORG_ESTRUCTURAL,\n",
    "     DISC_ADMINS.VTOW_WELL_LAST_CONTROL_DET VTOW_WELL_LAST_CONTROL_DET,\n",
    "     DISC_ADMINS.FIC_ULTIMO_DINAMOMETRO FIC_ULTIMO_DINAMOMETRO\n",
    "WHERE ((VTOW_WELL_LAST_CONTROL_DET.COMP_SK = DBU_FIC_ORG_ESTRUCTURAL.COMP_SK)\n",
    "       AND (FIC_ULTIMO_DINAMOMETRO.COMP_SK = DBU_FIC_ORG_ESTRUCTURAL.COMP_SK))\n",
    "  AND (DBU_FIC_ORG_ESTRUCTURAL.ESTADO = 'Produciendo')\n",
    "  AND ((FIC_ULTIMO_DINAMOMETRO.AIBRE_SOLICITACION_DE_ESTRUCT/100) <> 0)\n",
    "  AND ((FIC_ULTIMO_DINAMOMETRO.BBA_LLENADO_DE_BOMBA/100) <> 0)\n",
    "  AND (DBU_FIC_ORG_ESTRUCTURAL.MET_PROD = 'Bombeo Mecánico')\n",
    "\"\"\"\n",
    "\n",
    "SQL_PRODUCCION = \"\"\"\n",
    "SELECT UPPER(DBU_FIC_ORG_ESTRUCTURAL.NIVEL_4) AS NIVEL_4,\n",
    "       SUM(VTOW_WELL_LAST_CONTROL_DET.PROD_OIL) AS PROD_OIL_1\n",
    "FROM DISC_ADMINS.DBU_FIC_ORG_ESTRUCTURAL DBU_FIC_ORG_ESTRUCTURAL,\n",
    "     DISC_ADMINS.VTOW_WELL_LAST_CONTROL_DET VTOW_WELL_LAST_CONTROL_DET,\n",
    "     DISC_ADMINS.FIC_ULTIMO_DINAMOMETRO FIC_ULTIMO_DINAMOMETRO\n",
    "WHERE ((VTOW_WELL_LAST_CONTROL_DET.COMP_SK = DBU_FIC_ORG_ESTRUCTURAL.COMP_SK)\n",
    "       AND (FIC_ULTIMO_DINAMOMETRO.COMP_SK = DBU_FIC_ORG_ESTRUCTURAL.COMP_SK))\n",
    "  AND (DBU_FIC_ORG_ESTRUCTURAL.ESTADO = 'Produciendo')\n",
    "  AND ((FIC_ULTIMO_DINAMOMETRO.AIBRE_SOLICITACION_DE_ESTRUCT/100) <> 0)\n",
    "  AND ((FIC_ULTIMO_DINAMOMETRO.BBA_LLENADO_DE_BOMBA/100) <> 0)\n",
    "  AND (DBU_FIC_ORG_ESTRUCTURAL.MET_PROD = 'Bombeo Mecánico')\n",
    "GROUP BY UPPER(DBU_FIC_ORG_ESTRUCTURAL.NIVEL_4)\n",
    "\"\"\"\n",
    "\n",
    "SQL_ACCIONES = \"\"\"\n",
    "SELECT DISTINCT\n",
    "  DBU_FIC_ORG_ESTRUCTURAL.NIVEL_3,\n",
    "  FIC_ACCIONES.ACTIVIDAD,\n",
    "  FIC_ACCIONES.ESTADO,\n",
    "  FIC_ACCIONES.\"ESTADO AUTORIZACIÓN\",\n",
    "  FIC_ACCIONES.FECHAACCION,\n",
    "  FIC_ACCIONES.\"FECHA AUTORIZACION\",\n",
    "  FIC_ACCIONES.FECHAREALIZACION,\n",
    "  FIC_ACCIONES.JUSTIFICACION,\n",
    "  FIC_ACCIONES.OBJETIVO,\n",
    "  FIC_ACCIONES.OBSERVACION,\n",
    "  FIC_ACCIONES.ORIGEN,\n",
    "  FIC_ACCIONES.\"INCREMENTO BRUTA\",\n",
    "  FIC_ACCIONES.\"INCREMENTO PETRÓLEO\",\n",
    "  FIC_ACCIONES.RECURSO,\n",
    "  FIC_ACCIONES.SUBACTIVIDAD,\n",
    "  FIC_ACCIONES.\"USUARIO AUTORIZANTE\",\n",
    "  FIC_ACCIONES.\"USUARIO CREADOR\",\n",
    "  DBU_FIC_ORG_ESTRUCTURAL.NOMBRE_POZO,DBU_FIC_ORG_ESTRUCTURAL.NOMBRE_CORTO,\n",
    "  DBU_FIC_ORG_ESTRUCTURAL.NOMBRE_CORTO_POZO\n",
    "FROM DISC_ADMINS.DBU_FIC_ORG_ESTRUCTURAL DBU_FIC_ORG_ESTRUCTURAL,\n",
    "     DISC_ADMINS.FIC_ACCIONES FIC_ACCIONES\n",
    "WHERE (FIC_ACCIONES.CLAVEPOZO = DBU_FIC_ORG_ESTRUCTURAL.COMP_SK)\n",
    "  AND (DBU_FIC_ORG_ESTRUCTURAL.COMP_SK = FIC_ACCIONES.CLAVEPOZO)\n",
    "  AND (FIC_ACCIONES.FECHAACCION >= TO_DATE('20230101000000','YYYYMMDDHH24MISS'))\n",
    "  AND (DBU_FIC_ORG_ESTRUCTURAL.NIVEL_3 IN ('Las Heras CG - Canadon Escondida','Los Perales','El Guadal','Seco Leon - Pico Truncado'))\n",
    "\"\"\"\n",
    "\n",
    "# ================== CONEXIÓN ORACLE (cx_Oracle) ==================\n",
    "def get_connection():\n",
    "    dsn = cx_Oracle.makedsn(ORA_HOST, ORA_PORT, service_name=ORA_SERVICE)\n",
    "    return cx_Oracle.connect(ORA_USER, ORA_PASS, dsn, encoding=\"UTF-8\")\n",
    "\n",
    "def read_sql_df(conn, sql: str) -> pd.DataFrame:\n",
    "    return pd.read_sql(sql, con=conn)\n",
    "\n",
    "# ================== EXPO (auto-detección) ==================\n",
    "def load_exposicion(path: str, sheet: str | None) -> pd.DataFrame:\n",
    "    xls = pd.ExcelFile(path, engine=\"openpyxl\")\n",
    "    print(f\"Hojas encontradas en '{path}': {xls.sheet_names}\")\n",
    "\n",
    "    if sheet and sheet in xls.sheet_names:\n",
    "        df = pd.read_excel(xls, sheet_name=sheet)\n",
    "    else:\n",
    "        candidatos = [\"EXPOSICION\", \"EXPOSICIÓN\", \"Exposicion\", \"Hoja1\", \"Sheet1\"]\n",
    "        df = None\n",
    "        for s in candidatos:\n",
    "            if s in xls.sheet_names:\n",
    "                df = pd.read_excel(xls, sheet_name=s)\n",
    "                break\n",
    "        if df is None:\n",
    "            for s in xls.sheet_names:\n",
    "                tmp = pd.read_excel(xls, sheet_name=s)\n",
    "                cols_norm = [str(c).strip().lower() for c in tmp.columns]\n",
    "                if \"denominacion api\" in cols_norm:\n",
    "                    df = tmp\n",
    "                    print(f\"Usando hoja detectada por columna: {s}\")\n",
    "                    break\n",
    "        if df is None:\n",
    "            raise ValueError(f\"No pude encontrar la hoja de EXPOSICIÓN. Hojas: {xls.sheet_names}\")\n",
    "\n",
    "    if \"Cantidad\" in df.columns:\n",
    "        df = df.drop(columns=[\"Cantidad\"])\n",
    "\n",
    "    # Normalizar columna clave\n",
    "    col_api = None\n",
    "    for c in df.columns:\n",
    "        if str(c).strip().lower() == \"denominacion api\":\n",
    "            col_api = c; break\n",
    "    if col_api is None:\n",
    "        raise ValueError(\"No encuentro la columna 'Denominacion API' en la hoja seleccionada.\")\n",
    "\n",
    "    df[col_api] = df[col_api].astype(str).str.upper().str.strip()\n",
    "    df = df.rename(columns={col_api: \"Denominacion API\"})\n",
    "\n",
    "    for c in [\"Marca\",\"Tipo\",\"Torque max\",\"SENSIBILIDAD CR\",\"SENSIBILIDAD EST\"]:\n",
    "        if c not in df.columns:\n",
    "            df[c] = np.nan\n",
    "    return df\n",
    "\n",
    "# ================== Utilidades NLP/Reglas ==================\n",
    "_num = r\"(\\d+(?:[.,]\\d+)?)\"\n",
    "\n",
    "MODEL_WORDS = (\n",
    "    r\"(lufkin|vulcan|siam|pump ?jack|wuel?fel|darco|maxii?|weatherford|\"\n",
    "    r\"mel ?altium|altium|vc\\d{3}|lm[-\\s]?\\d+|m\\d{3}|c\\d{3}|rm[-\\s]?\\d+)\"\n",
    ")\n",
    "\n",
    "def _normalize(s):\n",
    "    if s is None or (isinstance(s, float) and np.isnan(s)): return \"\"\n",
    "    s = str(s)\n",
    "    s = unicodedata.normalize(\"NFKD\", s)\n",
    "    s = \"\".join(ch for ch in s if not unicodedata.combining(ch))\n",
    "    return s.lower()\n",
    "\n",
    "def _to_float(x):\n",
    "    try: return float(str(x).replace(\",\", \".\"))\n",
    "    except: return None\n",
    "\n",
    "def _any(txt, words):\n",
    "    t = _normalize(txt)\n",
    "    return any(w in t for w in words)\n",
    "\n",
    "def _has_up(txt):   return _any(txt, [\"increment\", \"aument\", \"subir\", \"sube\", \"suba\", \"+\"])\n",
    "def _has_down(txt): return _any(txt, [\"bajar\", \"disminu\", \"reducir\", \"baja\", \"-\"])\n",
    "\n",
    "def _numbers_trend_up(txt):\n",
    "    t = _normalize(txt)\n",
    "\n",
    "    m = re.search(r\"actual[^0-9]*\"+_num+r\".*final[^0-9]*\"+_num, t)\n",
    "    if m:\n",
    "        a,f = _to_float(m.group(1)), _to_float(m.group(2))\n",
    "        return None if a is None or f is None else f>a\n",
    "\n",
    "    m = re.search(r\"llevar[^0-9]*a[^0-9]*\"+_num+r\".*actual[^0-9]*\"+_num, t)\n",
    "    if m:\n",
    "        target,actual = _to_float(m.group(1)), _to_float(m.group(2))\n",
    "        return None if target is None or actual is None else target>actual\n",
    "\n",
    "    m = re.search(r\"sale[^0-9]*\"+_num+r\".*(entra|ingresa)[^0-9]*\"+_num, t)\n",
    "    if m:\n",
    "        s,e = _to_float(m.group(2)), _to_float(m.group(3))\n",
    "        return None if s is None or e is None else e>s\n",
    "\n",
    "    m = re.search(r\"\\bs[^0-9:]*[: ]\\s*\"+_num+r\".*\\be[^0-9:]*[: ]\\s*\"+_num, t)\n",
    "    if m:\n",
    "        s,e = _to_float(m.group(1)), _to_float(m.group(2))\n",
    "        return None if s is None or e is None else e>s\n",
    "\n",
    "    m = re.search(_num+r\"\\s*(rpm|gpm|hz|spm)\\D{0,15}(?:a|@|→|-?>)\\D{0,15}\"+_num, t)\n",
    "    if m:\n",
    "        a,f = _to_float(m.group(1)), _to_float(m.group(3))\n",
    "        return None if a is None or f is None else f>a\n",
    "\n",
    "    return None\n",
    "\n",
    "def _polea_direction(txt):\n",
    "    trend = _numbers_trend_up(txt)\n",
    "    if trend is None: return None\n",
    "    return \"SUBE REGIMEN\" if trend else \"BAJA REGIMEN\"\n",
    "\n",
    "def classify_accion(objetivo, observacion) -> str:\n",
    "    o   = _normalize(objetivo)\n",
    "    c   = _normalize(observacion)\n",
    "    txt = f\"{o} {c}\"\n",
    "\n",
    "    # --- A) CAMBIO CARRERA (prioritario sobre contrapesar)\n",
    "    if \"carrera\" in txt and re.search(r\"\\b(minim|maxim|intermed|bajar|subir|aumentar|disminuir|cambio de carrera|54|64|74|84|96|118|130|149|168|192)\\b\", txt):\n",
    "        return \"CAMBIO CARRERA\"\n",
    "\n",
    "    # --- B) CAMBIO AIB: verbos + AIB/modelo; “proveniente/locación/trasladar/tomar AIB”\n",
    "    if re.search(r\"\\b(instalar|montar|desmontar|transportar|intercambio|enroque|cambio|tomar)\\b.*\\b(aib|\"+MODEL_WORDS+r\")\\b\", txt) \\\n",
    "       or re.search(r\"\\b(proveniente de|locaci[oó]n|trasladar|mover|llevar)\\b.*\\b(aib|\"+MODEL_WORDS+r\")\\b\", txt):\n",
    "        return \"CAMBIO AIB\"\n",
    "\n",
    "    # --- C) CONTRAPESAR “puro”\n",
    "    if re.search(r\"\\b(mover|colocar|ubicar|retirar|balancear|balanceo|contrapesar)\\b.*\\b(contrapesos?|placas?)\\b\", txt):\n",
    "        return \"CONTRAPESAR\"\n",
    "\n",
    "    # --- D) SUBE/BAJA por evidencia numérica o de polea\n",
    "    trend = _numbers_trend_up(txt)\n",
    "    if trend is True:  base = \"SUBE REGIMEN\"\n",
    "    elif trend is False: base = \"BAJA REGIMEN\"\n",
    "    else: base = \"\"\n",
    "\n",
    "    if base == \"\" and (\"polea\" in txt or \"cambio de polea\" in txt):\n",
    "        polea = _polea_direction(txt)\n",
    "        if polea: base = polea\n",
    "        else:\n",
    "            base = \"SUBE REGIMEN\" if (\"optimizacion\" in o or \"optimización\" in objetivo.lower()) else (\"BAJA REGIMEN\" if \"operativa\" in o else \"SUBE REGIMEN\")\n",
    "\n",
    "    # verbos de subir/bajar (sin números claros)\n",
    "    if base == \"\":\n",
    "        if _has_up(txt)  and re.search(r\"\\b(regimen|gpm|rpm|hz|spm|mci|vsd|pid|pip)\\b\", txt):  base = \"SUBE REGIMEN\"\n",
    "        if _has_down(txt) and re.search(r\"\\b(regimen|gpm|rpm|hz|spm|mci)\\b\", txt):             base = \"BAJA REGIMEN\"\n",
    "\n",
    "    # --- E) Ajustes sin evidencia (llevar/estabilizar/ajustar/adecuar/setear X)\n",
    "    if base == \"\" and re.search(r\"\\b(llevar|estabilizar|ajustar|adecuar|sete(ar|o))\\b\", txt) and re.search(r\"\\b(gpm|rpm|hz|spm|mci|vsd)\\b\", txt):\n",
    "        base = \"BAJA REGIMEN\" if re.search(r\"\\b(golpe de fluido|gdf|gdf)\\b\", txt) else \"SUBE REGIMEN\"\n",
    "\n",
    "    # --- F) ACONDICIONAR (superficie)\n",
    "    if base == \"\":\n",
    "        if re.search(r\"\\b(hot ?oil|ho\\b|bache|batch|quimic|químic|dispersante|acido|ácido|solvente|freno|reparar freno|leuter|dispositivo|cubre ?polea|cubrepoleas?|rotador|medicion|medición|nivel|mf\\b|nf\\b|muestra|analisis|análisis|hermeticidad)\\b\", txt):\n",
    "            base = \"ACONDICIONAR EQUIP SUPERFICIE\"\n",
    "\n",
    "    # Guardrails\n",
    "    if \"optimizacion\" in o or \"optimización\" in objetivo.lower():\n",
    "        if base == \"BAJA REGIMEN\" or base == \"\": base = \"SUBE REGIMEN\"\n",
    "    if \"operativa\" in o:\n",
    "        strong_up = _has_up(txt) or trend is True or (_polea_direction(txt) == \"SUBE REGIMEN\")\n",
    "        if base == \"SUBE REGIMEN\" and not strong_up:\n",
    "            base = \"BAJA REGIMEN\"\n",
    "\n",
    "    return base or \"ACONDICIONAR EQUIP SUPERFICIE\"\n",
    "\n",
    "# ---- Re-etiquetado (capa post) ----\n",
    "def _is_contrapesar(txt):\n",
    "    return _any(txt, [\"contrapes\", \"placa\", \"balancear\", \"balanceo\", \"manivela\"])\n",
    "\n",
    "def _is_carrera_action(txt):\n",
    "    t = _normalize(txt)\n",
    "    return (\"carrera\" in t) and _any(t, [\n",
    "        \"cambio\", \"cambiar\", \"llevar\", \"pasar\", \"disminuir\", \"aumentar\",\n",
    "        \"minima\", \"mínima\", \"maxima\", \"máxima\", \"intermedia\", \"subir\", \"bajar\"\n",
    "    ])\n",
    "\n",
    "def _is_surface_conditioning(txt):\n",
    "    return _any(txt, [\n",
    "        \"hot oil\",\" ho\",\"bache\",\"batch\",\"quimic\",\"químic\",\"dispersante\",\n",
    "        \"acido\",\"ácido\",\"solvente\",\"freno\",\"leuter\",\"dispositivo\",\n",
    "        \"cubre\",\"cubrepolea\",\"cubre-polea\",\"rotador\",\"medicion\",\"medición\",\n",
    "        \"nivel\",\" mf\",\" nf\",\"muestra\",\"analisis\",\"análisis\",\"hermeticidad\"\n",
    "    ])\n",
    "\n",
    "def _is_polea_change(txt):\n",
    "    t = _normalize(txt)\n",
    "    if \"polea\" not in t: return False\n",
    "    return bool(re.search(r\"(sale|entra|ingresa|cambio de polea|cambiar polea|s:|e:|montar polea)\", t))\n",
    "\n",
    "def _has_model_movement(txt):\n",
    "    t = _normalize(txt)\n",
    "    movement = re.search(r\"\\b(mover|trasladar|transportar|llevar)\\b\", t)\n",
    "    model = re.search(r\"\\b(aib|\"+MODEL_WORDS+r\")\\b\", t)\n",
    "    return bool(movement and model)\n",
    "\n",
    "def reclassify_label(etiqueta_inicial: str, objetivo: str, observacion: str) -> str:\n",
    "    \"\"\"Aplica reglas de re-etiquetado sobre la etiqueta ya calculada (no cambia la lógica base).\"\"\"\n",
    "    etq = (etiqueta_inicial or \"\").strip().upper()\n",
    "    obj = _normalize(objetivo)\n",
    "    obs = _normalize(observacion)\n",
    "    txt = f\"{obj} {obs}\"\n",
    "\n",
    "    is_opt = (\"optimizacion\" in obj or \"optimización\" in objetivo.lower())\n",
    "    is_opr = (\"operativa\" in obj)\n",
    "\n",
    "    # 1) Si es ACONDICIONAR EQUIP SUPERFICIE\n",
    "    if etq == \"ACONDICIONAR EQUIP SUPERFICIE\":\n",
    "        # 1.a) contrapesar siempre gana\n",
    "        if _is_contrapesar(txt):\n",
    "            return \"CONTRAPESAR\"\n",
    "        # 1.b) cambio de polea: etiqueta por objetivo\n",
    "        if _is_polea_change(txt):\n",
    "            return \"SUBE REGIMEN\" if is_opt else \"BAJA REGIMEN\"\n",
    "        # 1.c) movimiento + modelo → CAMBIO AIB\n",
    "        if _has_model_movement(txt):\n",
    "            return \"CAMBIO AIB\"\n",
    "        # 1.d) GPM/RPM/Hz/SPM/MCI/VSD con verbos de ajuste (incluye setear)\n",
    "        if _any(txt, [\"gpm\",\"rpm\",\"hz\",\"spm\",\"mci\",\"vsd\"]) and _any(txt, [\"llevar\",\"estabilizar\",\"dejar\",\"ajustar\",\"adecuar\",\"setear\",\"seteo\"]):\n",
    "            return \"SUBE REGIMEN\" if is_opt else \"BAJA REGIMEN\"\n",
    "        return etq\n",
    "\n",
    "    # 2) Si es BAJA REGIMEN\n",
    "    if etq == \"BAJA REGIMEN\":\n",
    "        if _is_contrapesar(txt):\n",
    "            return \"CONTRAPESAR\"\n",
    "        if _is_carrera_action(txt):\n",
    "            return \"CAMBIO CARRERA\"\n",
    "        if _is_surface_conditioning(txt) and not _is_polea_change(txt):\n",
    "            return \"ACONDICIONAR EQUIP SUPERFICIE\"\n",
    "        # cambio de polea domina: BAJA si Operativa, SUBE si Opti\n",
    "        if _is_polea_change(txt):\n",
    "            return \"SUBE REGIMEN\" if is_opt else \"BAJA REGIMEN\"\n",
    "        return etq\n",
    "\n",
    "    # 3) Si es CAMBIO AIB\n",
    "    if etq == \"CAMBIO AIB\":\n",
    "        # 3.a) “montar polea” (no AIB) y objetivo Optimización → SUBE\n",
    "        if \"montar polea\" in txt and is_opt:\n",
    "            return \"SUBE REGIMEN\"\n",
    "        # 3.b) si NO hay verbos de AIB pero sí cambio de polea → por objetivo\n",
    "        verbos_aib = re.search(r\"\\b(instalar|montar|desmontar|transportar|intercambio|enroque|tomar)\\b\", txt)\n",
    "        if (not verbos_aib) and _is_polea_change(txt):\n",
    "            dir_ = _polea_direction(txt)\n",
    "            if dir_:\n",
    "                return dir_\n",
    "            return \"SUBE REGIMEN\" if is_opt else \"BAJA REGIMEN\"\n",
    "        return etq\n",
    "\n",
    "    # 4) Si es CAMBIO CARRERA\n",
    "    if etq == \"CAMBIO CARRERA\":\n",
    "        if re.search(r\"\\b(montar|instalar|transportar|trasladar|intercambio|proveniente|locaci[oó]n|enroque|tomar)\\b\", txt) and \\\n",
    "           _any(txt, [\"aib\",\"lufkin\",\"vulcan\",\"siam\",\"pump jack\",\"darco\",\"weatherford\",\"maxii\",\"wuel\",\"mel altium\",\"altium\"]):\n",
    "            return \"CAMBIO AIB\"\n",
    "        return etq\n",
    "\n",
    "    # 5) Si es CONTRAPESAR\n",
    "    if etq == \"CONTRAPESAR\":\n",
    "        if _is_carrera_action(txt):\n",
    "            return \"CAMBIO CARRERA\"\n",
    "        return etq\n",
    "\n",
    "    # 6) Si es SUBE REGIMEN\n",
    "    if etq == \"SUBE REGIMEN\":\n",
    "        if _is_carrera_action(txt):\n",
    "            return \"CAMBIO CARRERA\"\n",
    "        return etq\n",
    "\n",
    "    return etq\n",
    "\n",
    "# ================== Cargas base ==================\n",
    "def load_fdp(conn) -> pd.DataFrame:\n",
    "    df = read_sql_df(conn, SQL_FDP_DINA)\n",
    "    df[\"NIVEL_4\"] = df[\"NIVEL_4\"].astype(str).str.upper()\n",
    "    return df\n",
    "\n",
    "def load_produccion(conn) -> pd.DataFrame:\n",
    "    return read_sql_df(conn, SQL_PRODUCCION)\n",
    "\n",
    "def load_acciones(conn) -> pd.DataFrame:\n",
    "    df = read_sql_df(conn, SQL_ACCIONES)\n",
    "    for c in [\"FECHAACCION\",\"FECHAREALIZACION\",\"FECHA AUTORIZACION\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_datetime(df[c], errors=\"coerce\")\n",
    "    return df\n",
    "\n",
    "# ================== Transformaciones ==================\n",
    "def join_expo(df_fdp: pd.DataFrame, expo: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df_fdp.copy()\n",
    "    df[\"__API_KEY__\"] = df[\"AIB_MARCA_Y_DESC_API\"].astype(str).str.upper().str.strip()\n",
    "    ex = expo.copy()\n",
    "    ex[\"__API_KEY__\"] = ex[\"Denominacion API\"]\n",
    "\n",
    "    df = df.merge(\n",
    "        ex[[\"__API_KEY__\",\"Denominacion API\",\"Marca\",\"Tipo\",\"Torque max\",\"SENSIBILIDAD CR\",\"SENSIBILIDAD EST\"]],\n",
    "        how=\"left\", on=\"__API_KEY__\"\n",
    "    ).drop(columns=[\"__API_KEY__\"])\n",
    "\n",
    "    df = df.rename(columns={\n",
    "        \"Denominacion API\": \"EXPOSICIÓN.Denominacion API\",\n",
    "        \"Marca\": \"EXPOSICIÓN.Marca\",\n",
    "        \"Tipo\": \"EXPOSICIÓN.Tipo\",\n",
    "        \"Torque max\": \"EXPOSICIÓN.Torque max\",\n",
    "        \"SENSIBILIDAD CR\": \"EXPOSICIÓN.SENSIBILIDAD CR\",\n",
    "        \"SENSIBILIDAD EST\": \"EXPOSICIÓN.SENSIBILIDAD EST\",\n",
    "    })\n",
    "\n",
    "    df[\"ID_tmp\"] = (df[\"NOMBRE_CORTO\"].fillna(\"\").astype(str) + df[\"NIVEL_5\"].fillna(\"\").astype(str))\n",
    "    df = df[(df[\"ID_tmp\"]!=\"\") & df[\"NOMBRE_CORTO\"].notna()]\n",
    "    df = df.drop_duplicates(subset=[\"ID_tmp\"]).drop(columns=[\"ID_tmp\"])\n",
    "\n",
    "    for c in [\"AIBRR_TORQUE_MAXIMO_REDUCTOR\",\"AIBEB_TORQUE_MAXIMO_REDUCTOR\"]:\n",
    "        df[c] = df[c].fillna(0)\n",
    "\n",
    "    df[\"NIVEL_2\"] = df[\"NIVEL_2\"].replace({\"Tierra del Fuego\": \"Chubut\"})\n",
    "    return df\n",
    "\n",
    "def attach_produccion(df: pd.DataFrame, prod: pd.DataFrame) -> pd.DataFrame:\n",
    "    return df.merge(prod, how=\"left\", on=\"NIVEL_4\")\n",
    "\n",
    "def add_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    d = df.copy()\n",
    "    d[\"PROD_OIL_1\"] = d[\"PROD_OIL_1\"].replace({0: np.nan})\n",
    "    d[\"Ratio_Produccion\"] = d[\"PROD_OIL\"] / d[\"PROD_OIL_1\"]\n",
    "\n",
    "    def seg_consecuencia(x):\n",
    "        if pd.isna(x): return \"0 - 0,0025\"\n",
    "        if x >= 0.0065: return \"Mayor a 0,0065\"\n",
    "        if x >= 0.0025: return \"0,0025 - 0,0065\"\n",
    "        return \"0 - 0,0025\"\n",
    "    d[\"f_CONSECUENCIA\"] = d[\"Ratio_Produccion\"].apply(seg_consecuencia)\n",
    "    d[\"CONSECUENCIA\"] = d[\"f_CONSECUENCIA\"]\n",
    "\n",
    "    d[\"TORQUE_BAL[%]\"] = d[\"AIBEB_TORQUE_MAXIMO_REDUCTOR\"] / d[\"AIBRR_TORQUE_DISPONIBLE\"]\n",
    "    d[\"TORQUE[%]\"]     = d[\"AIBRR_TORQUE_MAXIMO_REDUCTOR\"] / d[\"AIBRR_TORQUE_DISPONIBLE\"]\n",
    "\n",
    "    d[\"f_CR\"]  = np.select([d[\"TORQUE[%]\"] > 1, d[\"TORQUE[%]\"] >= 0.85], [10, 3.7], default=0)\n",
    "    d[\"f_CR3\"] = d[\"EXPOSICIÓN.SENSIBILIDAD CR\"].map({\"ALTA\":3, \"MEDIA\":1.5}).fillna(1.0)\n",
    "    d[\"CAJA REDUCTORA\"]  = d[\"f_CR\"] * d[\"f_CR3\"]\n",
    "    d[\"CAJA REDUCTORA_\"] = np.where(d[\"TORQUE_BAL[%]\"]>1, 1.5*d[\"CAJA REDUCTORA\"], d[\"CAJA REDUCTORA\"])\n",
    "\n",
    "    d[\"f_estr\"]  = np.select([d[\"ESTRUCTURA_PCT\"] >= 0.95, d[\"ESTRUCTURA_PCT\"] > 0.85], [20,5], default=1)\n",
    "    d[\"f_estr3\"] = d[\"EXPOSICIÓN.SENSIBILIDAD EST\"].map({\"ALTA\":3,\"MEDIA\":1.5}).fillna(1.0)\n",
    "    d[\"ESTRUCTURA\"] = d[\"f_estr\"] * d[\"f_estr3\"]\n",
    "\n",
    "    d[\"f_GPM\"]  = np.select([d[\"AIB_GPM\"] > 9, d[\"AIB_GPM\"] > 7], [3, 1.5], default=1)\n",
    "    d[\"f_GPM2\"] = np.where(d[\"LLENAD_BOMBA_PCT\"] < 0.75, d[\"f_GPM\"]*2, d[\"f_GPM\"])\n",
    "\n",
    "    d[\"EXIGENCIA\"]   = d[\"f_GPM2\"] * d[\"ESTRUCTURA\"] + d[\"CAJA REDUCTORA_\"]\n",
    "    d[\"F_EXIGENCIA\"] = np.select([d[\"EXIGENCIA\"]>10, d[\"EXIGENCIA\"]>4], [\"A\",\"M\"], default=\"B\")\n",
    "\n",
    "    def crit(row):\n",
    "        rp = row[\"Ratio_Produccion\"]; fe = row[\"F_EXIGENCIA\"]\n",
    "        if pd.isna(rp) or pd.isna(fe): return \"NORMAL\"\n",
    "        if rp >= 0.0065 and fe == \"A\": return \"CRITICO\"\n",
    "        if rp >= 0.0025 and fe == \"A\": return \"ALERTA\"\n",
    "        if rp <  0.0025 and fe == \"A\": return \"ALERTA\"\n",
    "        if rp >= 0.0065 and fe == \"M\": return \"ALERTA\"\n",
    "        if rp >= 0.0025 and fe == \"M\": return \"ALERTA\"\n",
    "        if rp <  0.0025 and fe == \"M\": return \"ALERTA\"\n",
    "        return \"NORMAL\"\n",
    "    d[\"CRITICIDAD\"] = d.apply(crit, axis=1).astype(str)\n",
    "    d[\"ref_criticidad\"] = d[\"CRITICIDAD\"].map({\"CRITICO\":1, \"ALERTA\":2, \"NORMAL\":3}).astype(\"Int64\")\n",
    "\n",
    "    def detalle_crit(rp, fe):\n",
    "        if pd.isna(rp) or pd.isna(fe): return \"NORMAL\"\n",
    "        if rp >= 0.0065 and fe == \"A\": return \"AA\"\n",
    "        if rp >= 0.0025 and fe == \"A\": return \"AM\"\n",
    "        if rp <  0.0025 and fe == \"A\": return \"AB\"\n",
    "        if rp >= 0.0065 and fe == \"M\": return \"MA\"\n",
    "        if rp >= 0.0025 and fe == \"M\": return \"MM\"\n",
    "        if rp <  0.0025 and fe == \"M\": return \"MB\"\n",
    "        if rp >= 0.0065 and fe == \"B\": return \"BA\"\n",
    "        if rp >= 0.0025 and fe == \"B\": return \"BM\"\n",
    "        if rp <  0.0025 and fe == \"B\": return \"BB\"\n",
    "        return \"NORMAL\"\n",
    "    d[\"DETALLE CRITICIDAD\"] = np.vectorize(detalle_crit)(d[\"Ratio_Produccion\"], d[\"F_EXIGENCIA\"])\n",
    "\n",
    "    d[\"FECHA_HORA\"] = pd.to_datetime(d[\"FECHA_HORA\"], errors=\"coerce\")\n",
    "    max_por_pozo = d.groupby(\"NOMBRE_CORTO\", dropna=False)[\"FECHA_HORA\"].max().rename(\"FECHA_MAX\").reset_index()\n",
    "    d = d.merge(max_por_pozo, on=\"NOMBRE_CORTO\", how=\"left\")\n",
    "    d[\"Días sin dina\"] = (pd.Timestamp.today().normalize() - d[\"FECHA_MAX\"].dt.normalize()).dt.days\n",
    "\n",
    "    d[\"Correlativo Exigencia\"] = d[\"F_EXIGENCIA\"].map({\"A\":\"10 +\", \"M\":\"04-10\", \"B\":\"0-04\"}).fillna(\"\")\n",
    "    d[\"ID\"] = d[\"f_CONSECUENCIA\"].fillna(\"\").astype(str) + d[\"Correlativo Exigencia\"].fillna(\"\").astype(str)\n",
    "\n",
    "    d = d[(d[\"NOMBRE_CORTO\"].notna()) & (d[\"NOMBRE_CORTO\"].astype(str)!=\"\")]\n",
    "\n",
    "    return d\n",
    "\n",
    "# ================== PIPELINE ==================\n",
    "def main():\n",
    "    print(\"Conectando a Oracle (cx_Oracle)…\")\n",
    "    with get_connection() as conn:\n",
    "        print(\"Leyendo FDP_DINA…\")\n",
    "        fdp = load_fdp(conn)\n",
    "\n",
    "        print(\"Leyendo Producción…\")\n",
    "        prod = load_produccion(conn)\n",
    "\n",
    "        print(\"Leyendo ACCIONES…\")\n",
    "        acciones = load_acciones(conn)\n",
    "\n",
    "    print(\"Leyendo Excel EXPOSICIÓN…\")\n",
    "    expo = load_exposicion(EXPO_PATH, EXPO_SHEET)\n",
    "\n",
    "    print(\"Aplicando joins/limpieza (EXPOSICIÓN)…\")\n",
    "    fdp = join_expo(fdp, expo)\n",
    "\n",
    "    print(\"Adjuntando Producción…\")\n",
    "    fdp = attach_produccion(fdp, prod)\n",
    "\n",
    "    print(\"Calculando métricas / criticidad…\")\n",
    "    final = add_features(fdp)\n",
    "\n",
    "    # -------- (1) Subset de columnas solicitadas --------\n",
    "    columnas_matriz = [\n",
    "        \"NOMBRE_CORTO\",\n",
    "        \"FECHA_HORA\",\n",
    "        \"AIB_MARCA_Y_DESC_API\",\n",
    "        \"PROD_OIL\",\n",
    "        \"PROD_GAS\",\n",
    "        \"PROD_WAT\",\n",
    "        \"AIB_GPM\",\n",
    "        \"MOTOR_DIAMETRO_POLEA\",\n",
    "        \"EXPOSICIÓN.Denominacion API\",\n",
    "        \"CRITICIDAD\",\n",
    "        \"Días sin dina\",\n",
    "    ]\n",
    "    faltantes = [c for c in columnas_matriz if c not in final.columns]\n",
    "    if faltantes:\n",
    "        raise KeyError(f\"Faltan columnas en la matriz final: {faltantes}\")\n",
    "    matriz_subset = final[columnas_matriz].copy()\n",
    "\n",
    "    # -------- (2) Etiquetado de ACCIONES --------\n",
    "    for col in [\"OBJETIVO\", \"OBSERVACION\"]:\n",
    "        if col not in acciones.columns:\n",
    "            acciones[col] = \"\"\n",
    "\n",
    "    acciones[\"ETIQUETA_BASE\"] = acciones.apply(\n",
    "        lambda r: classify_accion(r.get(\"OBJETIVO\", \"\"), r.get(\"OBSERVACION\", \"\")),\n",
    "        axis=1\n",
    "    )\n",
    "    acciones[\"ETIQUETA\"] = acciones.apply(\n",
    "        lambda r: reclassify_label(r.get(\"ETIQUETA_BASE\", \"\"), r.get(\"OBJETIVO\", \"\"), r.get(\"OBSERVACION\", \"\")),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    \n",
    "        # ================== ACCIONES EN PROCESO / FINALIZADAS ==================\n",
    "    # ================== ACCIONES EN PROCESO / FINALIZADAS ==================\n",
    "    # Utilidades comunes\n",
    "    def _plural_dia(n):\n",
    "        try:\n",
    "            n = int(n)\n",
    "        except Exception:\n",
    "            return f\"{n} días\"\n",
    "        return f\"{n} día\" if n == 1 else f\"{n} días\"\n",
    "\n",
    "    def _fila_texto(obs, etq, dias, msg=\"sin hacerse la acción\"):\n",
    "        obs = (\"\" if pd.isna(obs) else str(obs)).strip()\n",
    "        etq = (\"\" if pd.isna(etq) else str(etq)).strip()\n",
    "        dias_txt = _plural_dia(dias if dias is not pd.NA else 0)\n",
    "        # sin punto final\n",
    "        return f\"{obs} -{etq} - Lleva {dias_txt} {msg}\"\n",
    "\n",
    "    def _acciones_texto_por_pozo(acciones_df: pd.DataFrame,\n",
    "                                 matriz_df: pd.DataFrame,\n",
    "                                 estados_validos: set,\n",
    "                                 col_salida: str,\n",
    "                                 msg_final: str = \"sin hacerse la acción\",\n",
    "                                 add_finalizada: bool = False,\n",
    "                                 date_tail: tuple[str, str, str] | None = None):\n",
    "        \"\"\"\n",
    "        Devuelve DF [NOMBRE_CORTO, col_salida] enumerado por pozo.\n",
    "\n",
    "        date_tail: (nombre_col_fecha, etiqueta_para_mostrar, formato_strftime)\n",
    "                   p.ej. (\"FECHAACCION\", \"FECHA ACCION\", \"%d/%m/%y\")\n",
    "                         (\"FECHAREALIZACION\", \"FECHA REALIZACION\", \"%d/%m/%Y\")\n",
    "        \"\"\"\n",
    "        acc = acciones_df.copy()\n",
    "\n",
    "        # Traer FECHA_HORA y CRITICIDAD desde MATRIZ\n",
    "        mat_cols_needed = [\"NOMBRE_CORTO\", \"FECHA_HORA\", \"CRITICIDAD\"]\n",
    "        mat_min = matriz_df[mat_cols_needed].drop_duplicates(\"NOMBRE_CORTO\")\n",
    "        acc = acc.merge(mat_min, on=\"NOMBRE_CORTO\", how=\"left\", suffixes=(\"\", \"_MATRIZ\"))\n",
    "\n",
    "        # Filtros base\n",
    "        crit_mask  = acc[\"CRITICIDAD\"].isin([\"CRITICO\", \"ALERTA\"])\n",
    "        fecha_mask = (pd.to_datetime(acc[\"FECHA_HORA\"], errors=\"coerce\")\n",
    "                      < pd.to_datetime(acc[\"FECHAACCION\"], errors=\"coerce\"))\n",
    "        act_mask   = ~acc[\"ACTIVIDAD\"].fillna(\"\").str.contains(\"Intervención de Fondo\", case=False, na=False)\n",
    "        obj_mask   = (\n",
    "            acc[\"OBJETIVO\"].fillna(\"\").str.contains(\"Operativa\", case=False, na=False) |\n",
    "            acc[\"OBJETIVO\"].fillna(\"\").str.contains(\"Optimización de producción\", case=False, na=False)\n",
    "        )\n",
    "\n",
    "        acc_fil = acc[crit_mask & fecha_mask & act_mask & obj_mask].copy()\n",
    "\n",
    "        # Estado\n",
    "        acc_fil = acc_fil[acc_fil[\"ESTADO\"].fillna(\"\").str.upper().isin(estados_validos)].copy()\n",
    "        if acc_fil.empty:\n",
    "            return pd.DataFrame(columns=[\"NOMBRE_CORTO\", col_salida])\n",
    "\n",
    "        # Texto + días desde FECHAACCION\n",
    "        today = pd.Timestamp.today().normalize()\n",
    "        acc_fil[\"FECHAACCION\"] = pd.to_datetime(acc_fil[\"FECHAACCION\"], errors=\"coerce\")\n",
    "        acc_fil[\"dias_sin\"] = (today - acc_fil[\"FECHAACCION\"].dt.normalize()).dt.days.clip(lower=0).astype(\"Int64\")\n",
    "\n",
    "        # Para “y lleva XXXX días finalizada.” usamos FECHAREALIZACION\n",
    "        if add_finalizada:\n",
    "            acc_fil[\"FECHAREALIZACION\"] = pd.to_datetime(acc_fil[\"FECHAREALIZACION\"], errors=\"coerce\")\n",
    "            acc_fil[\"dias_fin\"] = (today - acc_fil[\"FECHAREALIZACION\"].dt.normalize()).dt.days.clip(lower=0).astype(\"Int64\")\n",
    "            acc_fil[\"__extra__\"] = acc_fil[\"dias_fin\"].apply(\n",
    "                lambda d: f\" y lleva {_plural_dia(0 if pd.isna(d) else d)} finalizada.\"\n",
    "            )\n",
    "        else:\n",
    "            acc_fil[\"__extra__\"] = \"\"\n",
    "\n",
    "        # Sufijo de fecha (opcional) — p.ej. \" FECHA ACCION: 13/08/25\"\n",
    "        if date_tail:\n",
    "            col_fecha, etiqueta, fmt = date_tail\n",
    "            col_fecha_norm = pd.to_datetime(acc_fil[col_fecha], errors=\"coerce\")\n",
    "            acc_fil[\"__fecha_tail__\"] = col_fecha_norm.dt.strftime(fmt).fillna(\"\")\n",
    "            acc_fil[\"__fecha_tail__\"] = acc_fil[\"__fecha_tail__\"].apply(\n",
    "                lambda s: f\" {etiqueta}: {s}\" if s else \"\"\n",
    "            )\n",
    "        else:\n",
    "            acc_fil[\"__fecha_tail__\"] = \"\"\n",
    "\n",
    "        def _fila_texto(obs, etq, dias, extra=\"\", fecha_tail=\"\"):\n",
    "            obs = (\"\" if pd.isna(obs) else str(obs)).strip()\n",
    "            etq = (\"\" if pd.isna(etq) else str(etq)).strip()\n",
    "            dias_txt = _plural_dia(dias if dias is not pd.NA else 0)\n",
    "            return f\"{obs} -{etq} - Lleva {dias_txt} {msg_final}.{extra}{fecha_tail}\"\n",
    "\n",
    "        acc_fil[\"__texto__\"] = acc_fil.apply(\n",
    "            lambda r: _fila_texto(r[\"OBSERVACION\"], r[\"ETIQUETA\"], r[\"dias_sin\"],\n",
    "                                  r[\"__extra__\"], r[\"__fecha_tail__\"]),\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "        # Ordenar y enumerar\n",
    "        acc_fil = acc_fil.sort_values([\"NOMBRE_CORTO\", \"FECHAACCION\"], ascending=[True, True])\n",
    "\n",
    "        def _enumerar_series(s):\n",
    "            lst = list(s)\n",
    "            return \"\\n\".join(f\"{i}) {t}\" for i, t in enumerate(lst, start=1))\n",
    "\n",
    "        out = (\n",
    "            acc_fil.groupby(\"NOMBRE_CORTO\")[\"__texto__\"]\n",
    "            .apply(_enumerar_series)\n",
    "            .reset_index(name=col_salida)\n",
    "        )\n",
    "        return out\n",
    "\n",
    "    \n",
    "    # ---- Construir y mergear columnas en MATRIZ ----\n",
    "    \n",
    "    # 1) ACCIONES EN PROCESO  -> agrega \" FECHA ACCION: dd/mm/yy\"\n",
    "    estados_proceso = {\"EN PROCESO\", \"BORRADOR\", \"NO INICIADA\", \"PROPUESTO\"}\n",
    "    agg_proceso = _acciones_texto_por_pozo(\n",
    "        acciones, matriz_subset, estados_proceso, \"ACCIONES EN PROCESO\",\n",
    "        msg_final=\"sin hacerse la acción\",\n",
    "        add_finalizada=False,\n",
    "        date_tail=(\"FECHAACCION\", \"FECHA ACCION\", \"%d/%m/%y\")\n",
    "    )\n",
    "    if agg_proceso.empty:\n",
    "        agg_proceso = pd.DataFrame(columns=[\"NOMBRE_CORTO\", \"ACCIONES EN PROCESO\"])\n",
    "    matriz_subset = matriz_subset.merge(agg_proceso, on=\"NOMBRE_CORTO\", how=\"left\")\n",
    "    matriz_subset[\"ACCIONES EN PROCESO\"] = matriz_subset[\"ACCIONES EN PROCESO\"].fillna(\"\")\n",
    "\n",
    "    # 2) ACCIONES FINALIZADAS -> agrega “y lleva XXXX días finalizada.” y “ FECHA REALIZACION dd/mm/YYYY”\n",
    "    estados_finalizadas = {\"FINALIZADA\"}\n",
    "    agg_final = _acciones_texto_por_pozo(\n",
    "        acciones, matriz_subset, estados_finalizadas, \"ACCIONES FINALIZADAS\",\n",
    "        msg_final=\"iniciada la acción\",   # o “finalizada la acción” si preferís\n",
    "        add_finalizada=True,\n",
    "        date_tail=(\"FECHAREALIZACION\", \"FECHA REALIZACION\", \"%d/%m/%Y\")\n",
    "    )\n",
    "    if agg_final.empty:\n",
    "        agg_final = pd.DataFrame(columns=[\"NOMBRE_CORTO\", \"ACCIONES FINALIZADAS\"])\n",
    "    matriz_subset = matriz_subset.merge(agg_final, on=\"NOMBRE_CORTO\", how=\"left\")\n",
    "    matriz_subset[\"ACCIONES FINALIZADAS\"] = matriz_subset[\"ACCIONES FINALIZADAS\"].fillna(\"\")\n",
    "\n",
    "    # ================== Columna VER ==================\n",
    "    # Normalizamos textos (mayúsculas, NaN -> \"\")\n",
    "    proc = matriz_subset[\"ACCIONES EN PROCESO\"].fillna(\"\").str.upper()\n",
    "\n",
    "    # Soporta nombre singular o plural para finalizadas\n",
    "    if \"ACCIONES FINALIZADAS\" in matriz_subset.columns:\n",
    "        fin = matriz_subset[\"ACCIONES FINALIZADAS\"].fillna(\"\").str.upper()\n",
    "    elif \"ACCIONES FINALIZADA\" in matriz_subset.columns:\n",
    "        fin = matriz_subset[\"ACCIONES FINALIZADA\"].fillna(\"\").str.upper()\n",
    "    else:\n",
    "        fin = pd.Series(\"\", index=matriz_subset.index)\n",
    "\n",
    "    # 1) Proceso contiene: MATRIZ | CAMBIO AIB | CAMBIO CARRERA | CONTRAPESAR\n",
    "    m1 = proc.str.contains(r\"\\bMATRIZ\\b|CAMBIO AIB|CAMBIO CARRERA|CONTRAPESAR\", regex=True)\n",
    "\n",
    "    # 2) Proceso contiene: ACONDICIONAR EQUIP SUPERFICIE\n",
    "    m2 = proc.str.contains(\"ACONDICIONAR EQUIP SUPERFICIE\", regex=False)\n",
    "\n",
    "    # 3) Finalizadas contiene: MATRIZ | CAMBIO AIB | CAMBIO CARRERA | CONTRAPESAR | ACONDICIONAR EQUIP SUPERFICIE\n",
    "    m3 = fin.str.contains(r\"\\bMATRIZ\\b|CAMBIO AIB|CAMBIO CARRERA|CONTRAPESAR|ACONDICIONAR EQUIP SUPERFICIE\", regex=True)\n",
    "\n",
    "    matriz_subset[\"VER\"] = np.select(\n",
    "        [m1, m2, m3],\n",
    "        [\n",
    "            \"ACCION EN PROCESO PARA REGULARIZAR MATRIZ AIB\",\n",
    "            \"ACONDICIONAR SUPERFICIE PARA TOMAR MEDICION\",\n",
    "            \"ACTUALIZAR MEDICION DINAMOMETRICA\",\n",
    "        ],\n",
    "        default=\"\"\n",
    "    )\n",
    "\n",
    " \n",
    "    \n",
    "    # -------- Exportar a Excel con 2 hojas --------\n",
    "    out = Path(\"Matriz_AIB_export.xlsx\")\n",
    "    with pd.ExcelWriter(out, engine=\"openpyxl\") as writer:\n",
    "        matriz_subset.to_excel(writer, sheet_name=\"MATRIZ\", index=False)\n",
    "        acciones.to_excel(writer, sheet_name=\"ACCIONES\", index=False)\n",
    "\n",
    "    print(f\"✅ Exportado {len(matriz_subset):,} filas (MATRIZ) + {len(acciones):,} filas (ACCIONES) → {out.absolute()}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f245ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (CursoML-UDEMY)",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
